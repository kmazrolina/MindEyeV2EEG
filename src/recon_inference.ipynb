{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f16c9d4c-66cb-4692-a61d-9aa86a8765d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder, FrozenOpenCLIPEmbedder2\n",
    "from generative_models.sgm.models.diffusion import DiffusionEngine\n",
    "from generative_models.sgm.util import append_dims\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils\n",
    "from models import *\n",
    "\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e52985b1-95ff-487b-8b2d-cc1ad1c190b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: fmri_init_use_difussion_prior_2GPUS_1SUBJ\n",
      "--data_path=/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/data                     --ckpt_path=../train_logs/fmri_init_use_difussion_prior_2GPUS_1SUBJ/last.pth                     --model_name=fmri_init_use_difussion_prior_2GPUS_1SUBJ                     --subj=1                     --hidden_dim=1024                     --n_blocks=4                     --new_test                     --batch_size=4\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "plotting = False\n",
    "if utils.is_interactive():\n",
    "    model_name = \"fmri_init_use_difussion_prior_2GPUS_1SUBJ\"\n",
    "    print(\"model_name:\", model_name)\n",
    "\n",
    "    # other variables can be specified in the following string:\n",
    "    jupyter_args = f'--data_path=/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/data \\\n",
    "                    --ckpt_path=../train_logs/{model_name}/last.pth \\\n",
    "                    --model_name={model_name} \\\n",
    "                    --subj=1 \\\n",
    "                    --hidden_dim=1024 \\\n",
    "                    --n_blocks=4 \\\n",
    "                    --new_test \\\n",
    "                    --batch_size=4'\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49e5dae4-606d-4dc6-b420-df9e4c14737e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"will load ckpt for model found in ../train_logs/model_name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--cache_dir\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where misc. files downloaded from huggingface are stored. Defaults to current src directory.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to model checkpoint.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9,10],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=2048,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\",type=int,default=4,\n",
    ")\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "# make output directory\n",
    "os.makedirs(\"evals\",exist_ok=True)\n",
    "os.makedirs(f\"evals/{model_name}\",exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bebd73c-91bd-4bf3-924c-c5ddc4850998",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64672583-9f00-46f5-8d4e-00e4c7068a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms.functional import normalize\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "class Data4Model(torch.utils.data.Dataset):\n",
    "    def __init__(self, state='training', sub_id=1, transform=None):\n",
    "        \n",
    "        super(Data4Model, self).__init__()\n",
    "       \n",
    "        imgs = np.load(f'{data_path}/GetData/'+state+'_imgpaths.npy').tolist()\n",
    "        \n",
    "        eeg = np.load(f'{data_path}/PreprocessedEEG/sub-'+str(sub_id).zfill(2)+'/preprocessed_eeg_'+state+'.npy', allow_pickle=True).tolist()\n",
    "        eeg = eeg['preprocessed_eeg_data']\n",
    "        eeg = np.mean(eeg, axis=1)\n",
    "        eeg = eeg[:, np.r_[11:19, 43:52], 20:40] #take only 17 channels\n",
    "        self.imgs = imgs\n",
    "        self.eeg = torch.Tensor(eeg).to('cpu')\n",
    "        self.transform = transform\n",
    "  \n",
    "    def __len__(self):\n",
    "        return min(len(self.imgs), len(self.eeg))\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        imgs = self.transform(Image.open(self.imgs[item]).convert('RGB'))\n",
    "        eeg = torch.tensor(self.eeg[item])\n",
    "        return eeg, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1be099a1-54f1-4f4c-b9ff-efbadd16680c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 80, 63, 100)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "eeg = np.load(f'/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/data/PreprocessedEEG/sub-'+str(2).zfill(2)+'/preprocessed_eeg_'+'test'+'.npy', allow_pickle=True).tolist()\n",
    "eeg = eeg['preprocessed_eeg_data']\n",
    "eeg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "490cb029-6697-4627-a0cf-1bfcccc49818",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test dl for subject 1!\n",
      "\n",
      "test cases: 200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "test_data = Data4Model(state='test', sub_id=subj, transform=transform)\n",
    "num_test = len(test_data)\n",
    "test_dl = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    \n",
    "print(f\"Loaded test dl for subject {subj}!\\n\")\n",
    "print(f\"test cases: {num_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004d4168-9137-45a2-b6ed-33274d714193",
   "metadata": {},
   "source": [
    "### Check eeg input dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b899cdfe-570a-43e4-a1d5-a996214b3c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/tscratch/people/plgkzrobek/slurm_jobdir/1130387/tmp.t0033/ipykernel_247152/3381101559.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  eeg = torch.tensor(self.eeg[item])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG shape  torch.Size([4, 17, 20])\n",
      "eeg_input_dim  340\n",
      "hidden_dim 1024\n"
     ]
    }
   ],
   "source": [
    "eeg_example, _ = next(iter(test_dl))\n",
    "\n",
    "print(\"EEG shape \", eeg_example.shape)\n",
    "eeg_example = eeg_example.view(batch_size, -1) # batch_size x(im repetitions) x channels x  timepoints\n",
    "eeg_input_dim = eeg_example.shape[1]\n",
    "print(\"eeg_input_dim \", eeg_input_dim)\n",
    "print(\"hidden_dim\", hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72e31e6-f170-4426-b697-c8411ad67657",
   "metadata": {},
   "source": [
    "### Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3afc4858-b6a6-4a52-9303-b4a50ea5cc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n",
      "param counts:\n",
      "349,184 total\n",
      "349,184 trainable\n",
      "param counts:\n",
      "458,885,116 total\n",
      "458,885,116 trainable\n",
      "param counts:\n",
      "459,234,300 total\n",
      "459,234,300 trainable\n",
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "719,099,516 total\n",
      "719,099,500 trainable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "719099500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664\n",
    "\n",
    "if blurry_recon:\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    # Download the Autoencoder checkpoint from Hugging Face\n",
    "    autoenc_ckpt_path = hf_hub_download(repo_id='pscotti/mindeyev2', filename='sd_image_var_autoenc.pth', repo_type='dataset')\n",
    "    ckpt = torch.load(autoenc_ckpt_path)\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "\n",
    "\n",
    "class RidgeRegressionEEG(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_size, out_features, subj_list): \n",
    "        super(RidgeRegressionEEG, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for _ in subj_list\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "        \n",
    "model.ridge = RidgeRegressionEEG(eeg_input_dim, out_features=hidden_dim, subj_list=[subj])\n",
    "\n",
    "from diffusers.models.vae import Decoder\n",
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, \n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim) \n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# setup diffusion prior network\n",
    "out_dim = clip_emb_dim\n",
    "depth = 6\n",
    "dim_head = 52\n",
    "heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "timesteps = 100\n",
    "\n",
    "prior_network = PriorNetwork(\n",
    "        dim=out_dim,\n",
    "        depth=depth,\n",
    "        dim_head=dim_head,\n",
    "        heads=heads,\n",
    "        causal=False,\n",
    "        num_tokens = clip_seq_dim,\n",
    "        learned_query_mode=\"pos_emb\"\n",
    "    )\n",
    "\n",
    "model.diffusion_prior = BrainDiffusionPrior(\n",
    "    net=prior_network,\n",
    "    image_embed_dim=out_dim,\n",
    "    condition_on_text_encodings=False,\n",
    "    timesteps=timesteps,\n",
    "    cond_drop_prob=0.2,\n",
    "    image_embed_scale=None,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "utils.count_params(model.diffusion_prior)\n",
    "utils.count_params(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48561bf5-90d2-4843-a8ed-36a4c2779ba4",
   "metadata": {},
   "source": [
    "## Load pretrained model ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da76471d-37da-47b5-bba9-fccda7525780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---loading ../train_logs/fmri_init_use_difussion_prior_2GPUS_1SUBJ/last.pth ckpt---\n",
      "\n",
      "checkpoint loaded!\n"
     ]
    }
   ],
   "source": [
    "def load_ckpt(tag, ckpt_path, load_lr=True,load_optimizer=True,load_epoch=True,strict=True,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {ckpt_path} ckpt---\\n\")\n",
    "    checkpoint = torch.load(ckpt_path, map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint\n",
    "    print(f\"checkpoint loaded!\")\n",
    "\n",
    "load_ckpt(\"last\",ckpt_path, load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbeb1bd-891c-4cb4-8cfe-7c7d3e9adabb",
   "metadata": {},
   "source": [
    "## Setup text caption networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "295824db-ab3d-450c-90fb-f656e48994ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM\n",
    "from modeling_git import GitForCausalLMClipEmb\n",
    "inputs = processor(images=images, return_tensors=\"pt\")\n",
    "clip_text_model.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "clip_text_model.eval().requires_grad_(False)\n",
    "clip_text_seq_dim = 257\n",
    "clip_text_emb_dim = 1024\n",
    "\n",
    "class CLIPConverter(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPConverter, self).__init__()\n",
    "        self.linear1 = nn.Linear(clip_seq_dim, clip_text_seq_dim)\n",
    "        self.linear2 = nn.Linear(clip_emb_dim, clip_text_emb_dim)\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.linear1(x)\n",
    "        x = self.linear2(x.permute(0,2,1))\n",
    "        return x\n",
    "        \n",
    "clip_convert = CLIPConverter()\n",
    "clip_convert_ckpt_path = hf_hub_download(repo_id='pscotti/mindeyev2', filename='bigG_to_L_epoch8.pth', repo_type='dataset')\n",
    "state_dict = torch.load(clip_convert_ckpt_path, map_location='cpu')['model_state_dict']\n",
    "clip_convert.load_state_dict(state_dict, strict=True)\n",
    "clip_convert.to(device) # if you get OOM running this script, you can switch this to cpu and lower minibatch_size to 4\n",
    "del state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e81265ce-0a76-46bc-bcb6-60b00eb597e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitForCausalLMClipEmb(\n",
       "  (git): GitModelClipEmb(\n",
       "    (embeddings): GitEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (image_encoder): GitVisionModel(\n",
       "      (vision_model): GitVisionTransformer(\n",
       "        (embeddings): GitVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "          (position_embedding): Embedding(257, 1024)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): GitVisionEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-23): 24 x GitVisionEncoderLayer(\n",
       "              (self_attn): GitVisionAttention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): GitVisionMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder): GitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x GitLayer(\n",
       "          (attention): GitAttention(\n",
       "            (self): GitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): GitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): GitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): GitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_projection): GitProjection(\n",
       "      (visual_projection): Sequential(\n",
       "        (0): Linear(in_features=1024, out_features=768, bias=True)\n",
       "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=768, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clip_text_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0750e24-419d-4a89-a6fc-c6fa83eaac25",
   "metadata": {},
   "source": [
    "## Prep unCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f726f617-39f5-49e2-8d0c-d11d27d01c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 10. Setting context_dim to [1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n",
      "WARNING:sgm.modules.attention:SpatialTransformer: Found context dims [1664] of depth 1, which does not match the specified 'depth' of 2. Setting context_dim to [1664, 1664] now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedder #0: FrozenOpenCLIPImageEmbedder with 1909889025 params. Trainable: False\n",
      "Initialized embedder #1: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "Initialized embedder #2: ConcatTimestepEmbedderND with 0 params. Trainable: False\n",
      "vector_suffix torch.Size([1, 1024])\n"
     ]
    }
   ],
   "source": [
    "config = OmegaConf.load(\"generative_models/configs/unclip6.yaml\")\n",
    "config = OmegaConf.to_container(config, resolve=True)\n",
    "unclip_params = config[\"model\"][\"params\"]\n",
    "network_config = unclip_params[\"network_config\"]\n",
    "denoiser_config = unclip_params[\"denoiser_config\"]\n",
    "first_stage_config = unclip_params[\"first_stage_config\"]\n",
    "conditioner_config = unclip_params[\"conditioner_config\"]\n",
    "sampler_config = unclip_params[\"sampler_config\"]\n",
    "scale_factor = unclip_params[\"scale_factor\"]\n",
    "disable_first_stage_autocast = unclip_params[\"disable_first_stage_autocast\"]\n",
    "offset_noise_level = unclip_params[\"loss_fn_config\"][\"params\"][\"offset_noise_level\"]\n",
    "\n",
    "first_stage_config['target'] = 'sgm.models.autoencoder.AutoencoderKL'\n",
    "sampler_config['params']['num_steps'] = 38\n",
    "\n",
    "diffusion_engine = DiffusionEngine(network_config=network_config,\n",
    "                       denoiser_config=denoiser_config,\n",
    "                       first_stage_config=first_stage_config,\n",
    "                       conditioner_config=conditioner_config,\n",
    "                       sampler_config=sampler_config,\n",
    "                       scale_factor=scale_factor,\n",
    "                       disable_first_stage_autocast=disable_first_stage_autocast)\n",
    "# set to inference\n",
    "diffusion_engine.eval().requires_grad_(False)\n",
    "diffusion_engine.to(device)\n",
    "\n",
    "diffusion_engine_ckpt_path = hf_hub_download(repo_id='pscotti/mindeyev2', filename='unclip6_epoch0_step110000.ckpt', repo_type='dataset')\n",
    "ckpt = torch.load(diffusion_engine_ckpt_path, map_location='cpu')\n",
    "diffusion_engine.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "batch={\"jpg\": torch.randn(1,3,1,1).to(device), # jpg doesnt get used, it's just a placeholder\n",
    "      \"original_size_as_tuple\": torch.ones(1, 2).to(device) * 768,\n",
    "      \"crop_coords_top_left\": torch.zeros(1, 2).to(device)}\n",
    "out = diffusion_engine.conditioner(batch)\n",
    "vector_suffix = out[\"vector\"].to(device)\n",
    "print(\"vector_suffix\", vector_suffix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2314d22a-aba5-4006-930c-5ff24e7a2801",
   "metadata": {},
   "source": [
    "# Make Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c6a706a3-d151-4643-bb34-7d08aa7361c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/net/tscratch/people/plgkzrobek/slurm_jobdir/1130387/tmp.t0033/ipykernel_247152/1282227072.py:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  eeg = torch.tensor(self.eeg[item])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2b5f28260f466ab2829b871a855969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sampling loop time step:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:01<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 256, 1664])\n",
      "torch.Size([4, 3, 224, 224])\n",
      "torch.Size([4, 257, 1024])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'err' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 49\u001b[0m\n\u001b[1;32m     47\u001b[0m pred_caption_emb \u001b[38;5;241m=\u001b[39m clip_convert(prior_out)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(pred_caption_emb\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 49\u001b[0m \u001b[43merr\u001b[49m\n\u001b[1;32m     50\u001b[0m generated_ids \u001b[38;5;241m=\u001b[39m clip_text_model\u001b[38;5;241m.\u001b[39mgenerate(pixel_values\u001b[38;5;241m=\u001b[39mpred_caption_emb, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m)\n\u001b[1;32m     51\u001b[0m generated_caption \u001b[38;5;241m=\u001b[39m processor\u001b[38;5;241m.\u001b[39mbatch_decode(generated_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'err' is not defined"
     ]
    }
   ],
   "source": [
    "# get all reconstructions\n",
    "model.to(device)\n",
    "model.eval().requires_grad_(False)\n",
    "\n",
    "all_images = None\n",
    "all_blurryrecons = None\n",
    "all_recons = None\n",
    "all_predcaptions = []\n",
    "all_captions = []\n",
    "all_clipeeg = None\n",
    "\n",
    "minibatch_size = 1\n",
    "num_samples_per_image = 1\n",
    "assert num_samples_per_image == 1\n",
    "\n",
    "if utils.is_interactive(): plotting=True\n",
    "\n",
    "with torch.no_grad(), torch.cuda.amp.autocast(dtype=torch.bfloat16):\n",
    "    for eeg, imgs in tqdm(test_dl):\n",
    "\n",
    "        eeg = eeg.to(device) \n",
    "        image = imgs.to(device)\n",
    "        eeg_ridge = model.ridge(eeg.view(batch_size,-1).unsqueeze(1), 0)\n",
    "\n",
    "        backbone, clip_eeg, blurry_image_enc = model.backbone(eeg_ridge)\n",
    "\n",
    "        #Save ground tuth images\n",
    "        if all_images is None:\n",
    "            all_images = image.cpu()\n",
    "        else:\n",
    "            all_images = torch.vstack((all_images,image.cpu() ))\n",
    "\n",
    "        # Save retrieval submodule outputs\n",
    "        if all_clipeeg is None:\n",
    "            all_clipeeg = clip_eeg.cpu()\n",
    "        else:\n",
    "            all_clipeeg = torch.vstack((all_clipeeg, clip_eeg.cpu()))\n",
    "        \n",
    "        # Feed eeg through OpenCLIP-bigG diffusion prior\n",
    "        prior_out = model.diffusion_prior.p_sample_loop(backbone.shape, \n",
    "                        text_cond = dict(text_embed = backbone), \n",
    "                        cond_scale = 1., timesteps = 20)\n",
    "\n",
    "        pred_caption_emb = clip_convert(prior_out)\n",
    "        generated_ids = clip_text_model.generate(pixel_values=pred_caption_emb, max_length=20)\n",
    "        generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "        all_predcaptions = np.hstack((all_predcaptions, generated_caption))\n",
    "        print(\"Generated captions: \", generated_caption)\n",
    "\n",
    "        \n",
    "        # Feed diffusion prior outputs through unCLIP\n",
    "        for i in range(len(eeg)):\n",
    "            samples = utils.unclip_recon(prior_out[[i]],\n",
    "                             diffusion_engine,\n",
    "                             vector_suffix,\n",
    "                             num_samples=num_samples_per_image)\n",
    "            if all_recons is None:\n",
    "                all_recons = samples.cpu()\n",
    "            else:\n",
    "                all_recons = torch.vstack((all_recons, samples.cpu()))\n",
    "            if plotting:\n",
    "                for s in range(num_samples_per_image):\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.title(generated_caption[i])\n",
    "                    plt.imshow(transforms.ToPILImage()(samples[s]))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "        if blurry_recon:\n",
    "            image_enc_pred, _ = blurry_image_enc\n",
    "            blurred_image = (autoenc.decode(image_enc_pred/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "            \n",
    "            for i in range(len(eeg)):\n",
    "                im = torch.Tensor(blurred_image[i])\n",
    "                if all_blurryrecons is None:\n",
    "                    all_blurryrecons = im[None].cpu()\n",
    "                else:\n",
    "                    all_blurryrecons = torch.vstack((all_blurryrecons, im[None].cpu()))\n",
    "                if plotting:\n",
    "                    plt.figure(figsize=(2,2))\n",
    "                    plt.title(f\"blurry recon: {generated_caption[i]}\")\n",
    "                    plt.imshow(transforms.ToPILImage()(im))\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "        if plotting: \n",
    "            print(model_name)\n",
    "            #break # dont actually want to run the whole thing with plotting=True\n",
    "\n",
    "if not utils.is_interactive():\n",
    "    # resize outputs before saving\n",
    "    imsize = 256\n",
    "    all_recons = transforms.Resize((imsize,imsize))(all_recons).float()\n",
    "    if blurry_recon: \n",
    "        all_blurryrecons = transforms.Resize((imsize,imsize))(all_blurryrecons).float()\n",
    "            \n",
    "    # saving\n",
    "    print(\"all_recons.shape: \", all_recons.shape)\n",
    "    print(\"all_images.shape: \",all_images.shape)\n",
    "    torch.save(all_images,\"evals/all_images.pt\") \n",
    "    torch.save(all_captions,\"evals/all_captions.pt\") \n",
    "    if blurry_recon:\n",
    "        torch.save(all_blurryrecons,f\"evals/{model_name}/{model_name}_all_blurryrecons.pt\")\n",
    "    torch.save(all_recons,f\"evals/{model_name}/{model_name}_all_recons.pt\")\n",
    "    torch.save(all_predcaptions,f\"evals/{model_name}/{model_name}_all_predcaptions.pt\")\n",
    "    torch.save(all_clipeeg,f\"evals/{model_name}/{model_name}_all_clipeeg.pt\")\n",
    "    print(f\"saved {model_name} outputs!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
