{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f0f4f3",
   "metadata": {},
   "source": [
    "# Import packages & functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bad764b-45c1-45ce-a716-8d055e09821a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import numpy as np\n",
    "import math\n",
    "from einops import rearrange\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "import webdataset as wds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from accelerate import Accelerator\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# SDXL unCLIP requires code from https://github.com/Stability-AI/generative-models/tree/main\n",
    "sys.path.append('generative_models/')\n",
    "import sgm\n",
    "from generative_models.sgm.modules.encoders.modules import FrozenOpenCLIPImageEmbedder # bigG embedder\n",
    "\n",
    "# tf32 data type is faster than standard float32\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "# custom functions #\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc5d2e32-6027-4a19-bef4-5ca068db35bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOCAL RANK  0\n"
     ]
    }
   ],
   "source": [
    "### Multi-GPU config ###\n",
    "local_rank = os.getenv('RANK')\n",
    "if local_rank is None: \n",
    "    local_rank = 0\n",
    "else:\n",
    "    local_rank = int(local_rank)\n",
    "print(\"LOCAL RANK \", local_rank)  \n",
    "\n",
    "data_type = torch.float16 # change depending on your mixed_precision\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0: num_devices = 1\n",
    "\n",
    "# First use \"accelerate config\" in terminal and setup using deepspeed stage 2 with CPU offloading!\n",
    "accelerator = Accelerator(split_batches=False, mixed_precision=\"fp16\")\n",
    "if utils.is_interactive(): # set batch size here if using interactive notebook instead of submitting job\n",
    "    global_batch_size = batch_size = 8\n",
    "else:\n",
    "    global_batch_size = os.environ[\"GLOBAL_BATCH_SIZE\"]\n",
    "    batch_size = int(os.environ[\"GLOBAL_BATCH_SIZE\"]) // num_devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08a02192-141f-42f7-acc9-7f7df0acf5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(num_devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b767ab6f-d4a9-47a5-b3bf-f56bf6760c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PID of this process = 738084\n",
      "device: cuda\n",
      "Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "distributed = False num_devices = 1 local rank = 0 world size = 1 data_type = torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(\"PID of this process =\",os.getpid())\n",
    "device = accelerator.device\n",
    "print(\"device:\",device)\n",
    "world_size = accelerator.state.num_processes\n",
    "distributed = not accelerator.state.distributed_type == 'NO'\n",
    "num_devices = torch.cuda.device_count()\n",
    "if num_devices==0 or not distributed: num_devices = 1\n",
    "num_workers = num_devices\n",
    "print(accelerator.state)\n",
    "\n",
    "print(\"distributed =\",distributed, \"num_devices =\", num_devices, \"local rank =\", local_rank, \"world size =\", world_size, \"data_type =\", data_type)\n",
    "print = accelerator.print # only print if local_rank=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018b82b-c054-4463-9527-4b0c2a75bda6",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b61fec7-72a0-4b67-86da-1375f1d9fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: eeg_multisubject_fmri_init\n",
      "--data_path=/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/data --model_name=eeg_multisubject_fmri_init --multi_subject --batch_size=16 --max_lr=3e-4 --mixup_pct=.33 --num_epochs=2 --use_prior --prior_scale=30 --clip_scale=1 --blur_scale=.5 --no-use_image_aug --n_blocks=4 --hidden_dim=1024 --ckpt_interval=999 --ckpt_saving --wandb_log --preprocessing_method=THINGSEEG2 --multisubject_ckpt=/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/train_logs/multisubject_subj01_1024hid_nolow_300ep\n"
     ]
    }
   ],
   "source": [
    "# if running this interactively, can specify jupyter_args here for argparser to use\n",
    "if utils.is_interactive():\n",
    "    model_name = \"eeg_multisubject_fmri_init\"\n",
    "    print(\"model_name:\", model_name)\n",
    "    \n",
    "    # global_batch_size and batch_size should already be defined in the 2nd cell block\n",
    "    jupyter_args = f\"--data_path=/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/data \\\n",
    "--model_name={model_name} \\\n",
    "--multi_subject \\\n",
    "--batch_size=16 \\\n",
    "--max_lr=3e-4 \\\n",
    "--mixup_pct=.33 \\\n",
    "--num_epochs=2 \\\n",
    "--use_prior \\\n",
    "--prior_scale=30 \\\n",
    "--clip_scale=1 \\\n",
    "--blur_scale=.5 \\\n",
    "--no-use_image_aug \\\n",
    "--n_blocks=4 \\\n",
    "--hidden_dim=1024 \\\n",
    "--ckpt_interval=999 \\\n",
    "--ckpt_saving \\\n",
    "--wandb_log \\\n",
    "--preprocessing_method=THINGSEEG2 \\\n",
    "--multisubject_ckpt=/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/train_logs/multisubject_subj01_1024hid_nolow_300ep\"\n",
    "    print(jupyter_args)\n",
    "    jupyter_args = jupyter_args.split()\n",
    "    \n",
    "    from IPython.display import clear_output # function to clear print outputs in cell\n",
    "    %load_ext autoreload \n",
    "    # this allows you to change functions in models.py or utils.py and have this notebook automatically update with your revisions\n",
    "    %autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2028bdf0-2f41-46d9-b6e7-86b870dbf16c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subj_list [ 2  3  4  5  6  7  8  9 10]\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Model Training Configuration\")\n",
    "parser.add_argument(\n",
    "    \"--model_name\", type=str, default=\"testing\",\n",
    "    help=\"name of model, used for ckpt saving and wandb logging (if enabled)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--data_path\", type=str, default=os.getcwd(),\n",
    "    help=\"Path to where NSD data is stored / where to download it to\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--subj\",type=int, default=1, choices=[1,2,3,4,5,6,7,8,9],\n",
    "    help=\"Validate on which subject?\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multisubject_ckpt\", type=str, default=None,\n",
    "    help=\"Path to pre-trained multisubject model to finetune a single subject from. multisubject must be False.\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_prior\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to train diffusion prior (True) or just rely on retrieval part of the pipeline (False)\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--batch_size\", type=int, default=16,\n",
    "    help=\"Batch size can be increased by 10x if only training retreival submodule and not diffusion prior\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_log\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to log to wandb\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--wandb_project\",type=str,default=\"stability\",\n",
    "    help=\"wandb project name\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--mixup_pct\",type=float,default=.33,\n",
    "    help=\"proportion of way through training when to switch from BiMixCo to SoftCLIP\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blurry_recon\",action=argparse.BooleanOptionalAction,default=True,\n",
    "    help=\"whether to output blurry reconstructions\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--blur_scale\",type=float,default=.5,\n",
    "    help=\"multiply loss from blurry recons by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--clip_scale\",type=float,default=1.,\n",
    "    help=\"multiply contrastive loss by this number\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--prior_scale\",type=float,default=30,\n",
    "    help=\"multiply diffusion prior loss by this\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--use_image_aug\",action=argparse.BooleanOptionalAction,default=False,\n",
    "    help=\"whether to use image augmentation\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--num_epochs\",type=int,default=150,\n",
    "    help=\"number of epochs of training\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--multi_subject\",action=argparse.BooleanOptionalAction,default=False,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--new_test\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--n_blocks\",type=int,default=4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--hidden_dim\",type=int,default=1024,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--lr_scheduler_type\",type=str,default='cycle',choices=['cycle','linear'],\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_saving\",action=argparse.BooleanOptionalAction,default=True,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--ckpt_interval\",type=int,default=5,\n",
    "    help=\"save backup ckpt and reconstruct every x epochs\",\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--seed\",type=int,default=42,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--max_lr\",type=float,default=3e-4,\n",
    ")\n",
    "parser.add_argument(\n",
    "    \"--preprocessing_method\",type=str,default=\"unspecified\",\n",
    ")\n",
    "\n",
    "if utils.is_interactive():\n",
    "    args = parser.parse_args(jupyter_args)\n",
    "else:\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# create global variables without the args prefix\n",
    "for attribute_name in vars(args).keys():\n",
    "    globals()[attribute_name] = getattr(args, attribute_name)\n",
    "    \n",
    "# seed all random functions\n",
    "utils.seed_everything(seed)\n",
    "\n",
    "outdir = os.path.abspath(f'../train_logs/{model_name}')\n",
    "if not os.path.exists(outdir) and ckpt_saving:\n",
    "    os.makedirs(outdir,exist_ok=True)\n",
    "    \n",
    "if use_image_aug or blurry_recon:\n",
    "    import kornia\n",
    "    from kornia.augmentation.container import AugmentationSequential\n",
    "if use_image_aug:\n",
    "    img_augment = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1, p=0.3),\n",
    "        same_on_batch=False,\n",
    "        data_keys=[\"input\"],\n",
    "    )\n",
    "    \n",
    "if multi_subject:\n",
    "    subj_list = np.arange(1,11)\n",
    "    subj_list = subj_list[subj_list != subj]\n",
    "else:\n",
    "    subj_list = [subj]\n",
    "\n",
    "print(\"subj_list\", subj_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d13c25-1369-4c49-81d4-83d713586096",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prep data, models, and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea8ea4-23bb-4ba7-bc40-dcc54d8f3cff",
   "metadata": {},
   "source": [
    "### Prepare DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8150366c",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "class Data4Model(torch.utils.data.Dataset):\n",
    "    def __init__(self, state='training', sub_id=1, transform=None):\n",
    "        \n",
    "        super(Data4Model, self).__init__()\n",
    "       \n",
    "        imgs = np.load(f'{data_path}/GetData/'+state+'_imgpaths.npy').tolist()\n",
    "        \n",
    "        eeg = np.load(f'{data_path}/PreprocessedEEG/sub-'+str(sub_id).zfill(2)+'/preprocessed_eeg_'+state+'.npy', allow_pickle=True).tolist()\n",
    "        eeg = eeg['preprocessed_eeg_data']\n",
    "        eeg = np.mean(eeg, axis=1)\n",
    "        \n",
    "        self.imgs = imgs\n",
    "        self.eeg = eeg\n",
    "        self.transform = transform\n",
    "  \n",
    "    def __len__(self):\n",
    "        return min(len(self.imgs), len(self.eeg))\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        imgs = self.transform(Image.open(self.imgs[item]).convert('RGB'))\n",
    "        eeg = torch.tensor(self.eeg[item]).float()\n",
    "        return eeg, imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81084834-035f-4465-ad59-59e6b806a2f5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded test and train dl for subject 1!\n",
      "\n",
      "test cases: 200 and train cases: 16540\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "num_test = 200\n",
    "num_train = 16540\n",
    "\n",
    "train_dls = []\n",
    "test_dls = []\n",
    "\n",
    "for sub_id in subj_list:\n",
    "\n",
    "    train_data = Data4Model(state='training', sub_id=sub_id, transform=transform)\n",
    "    train_dl = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    test_data = Data4Model(state='test', sub_id=sub_id, transform=transform)\n",
    "    test_dl = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=False, drop_last=True, pin_memory=True)\n",
    "    \n",
    "    train_dls.append(train_dl)\n",
    "    test_dls.append(test_dl)\n",
    "    \n",
    "    print(f\"Loaded test and train dl for subject {sub_id}!\\n\")\n",
    "    print(f\"test cases: {len(test_data)} and train cases: {len(train_data)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec4517-dbdf-4ece-98f6-4714d5de4e15",
   "metadata": {},
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d6160e-1ee8-4da7-a755-9dbb452a6fa5",
   "metadata": {},
   "source": [
    "### CLIP image embeddings  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0420dc0-199e-4c1a-857d-b1747058b467",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "clip_img_embedder = FrozenOpenCLIPImageEmbedder(\n",
    "    arch=\"ViT-bigG-14\",\n",
    "    version=\"laion2b_s39b_b160k\",\n",
    "    output_tokens=True,\n",
    "    only_tokens=True,\n",
    ")\n",
    "clip_img_embedder.to(device)\n",
    "\n",
    "clip_seq_dim = 256\n",
    "clip_emb_dim = 1664"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b79bd38-6990-4504-8d45-4a68d57d8885",
   "metadata": {},
   "source": [
    "### SD VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01baff79-8114-482b-b115-6f05aa8ad691",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "83,653,863 total\n",
      "0 trainable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "if blurry_recon:\n",
    "    from diffusers import AutoencoderKL    \n",
    "    autoenc = AutoencoderKL(\n",
    "        down_block_types=['DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D', 'DownEncoderBlock2D'],\n",
    "        up_block_types=['UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D', 'UpDecoderBlock2D'],\n",
    "        block_out_channels=[128, 256, 512, 512],\n",
    "        layers_per_block=2,\n",
    "        sample_size=256,\n",
    "    )\n",
    "    # Download the Autoencoder checkpoint from Hugging Face\n",
    "    autoenc_ckpt_path = hf_hub_download(repo_id='pscotti/mindeyev2', filename='sd_image_var_autoenc.pth', repo_type='dataset')\n",
    "    ckpt = torch.load(autoenc_ckpt_path)\n",
    "    autoenc.load_state_dict(ckpt)\n",
    "    \n",
    "    autoenc.eval()\n",
    "    autoenc.requires_grad_(False)\n",
    "    autoenc.to(device)\n",
    "    utils.count_params(autoenc)\n",
    "    \n",
    "    from autoencoder.convnext import ConvnextXL\n",
    "    # Download ConvnextXL checkpoint from Hugging Face\n",
    "    cnx_ckpt_path = hf_hub_download(repo_id='pscotti/mindeyev2', filename='convnext_xlarge_alpha0.75_fullckpt.pth', repo_type='dataset')\n",
    "    cnx = ConvnextXL(cnx_ckpt_path)\n",
    "    cnx.requires_grad_(False)\n",
    "    cnx.eval()\n",
    "    cnx.to(device)\n",
    "    \n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).to(device).reshape(1,3,1,1)\n",
    "    std = torch.tensor([0.228, 0.224, 0.225]).to(device).reshape(1,3,1,1)\n",
    "    \n",
    "    blur_augs = AugmentationSequential(\n",
    "        kornia.augmentation.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.2, hue=0.1, p=0.8),\n",
    "        kornia.augmentation.RandomGrayscale(p=0.1),\n",
    "        kornia.augmentation.RandomSolarize(p=0.1),\n",
    "        kornia.augmentation.RandomResizedCrop((224,224), scale=(.9,.9), ratio=(1,1), p=1.0),\n",
    "        data_keys=[\"input\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260e5e4a-f697-4b2c-88fc-01f6a54886c0",
   "metadata": {},
   "source": [
    "### MindEye modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c44c271b-173f-472e-b059-a2eda0f4c4c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MindEyeModule()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MindEyeModule(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MindEyeModule, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        \n",
    "model = MindEyeModule()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b8de65a-6d3b-4248-bea9-9b6f4d562321",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "458,885,116 total\n",
      "458,885,116 trainable\n",
      "param counts:\n",
      "458,885,116 total\n",
      "458,885,116 trainable\n",
      "b.shape torch.Size([2, 1, 1024])\n",
      "torch.Size([2, 256, 1664]) torch.Size([2, 256, 1664]) torch.Size([2, 4, 28, 28]) torch.Size([2, 49, 512])\n"
     ]
    }
   ],
   "source": [
    "from models import BrainNetwork\n",
    "model.backbone = BrainNetwork(h=hidden_dim, in_dim=hidden_dim, seq_len=1, n_blocks=n_blocks,\n",
    "                          clip_size=clip_emb_dim, out_dim=clip_emb_dim*clip_seq_dim, \n",
    "                          blurry_recon=blurry_recon, clip_scale=clip_scale)\n",
    "utils.count_params(model.backbone)\n",
    "utils.count_params(model)\n",
    "\n",
    "# test that the model works on some fake data\n",
    "b = torch.randn((2,1,hidden_dim))\n",
    "print(\"b.shape\",b.shape)\n",
    "\n",
    "backbone_, clip_, blur_ = model.backbone(b)\n",
    "print(backbone_.shape, clip_.shape, blur_[0].shape, blur_[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397c0d7-52a3-4153-823b-c27d2eb3eeba",
   "metadata": {},
   "source": [
    "### Adding diffusion prior + unCLIP if use_prior=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "69965344-9346-4592-9cc5-e537e31d5fce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "259,865,216 total\n",
      "259,865,200 trainable\n",
      "param counts:\n",
      "718,750,332 total\n",
      "718,750,316 trainable\n"
     ]
    }
   ],
   "source": [
    "if use_prior:\n",
    "    from models import *\n",
    "\n",
    "    # setup diffusion prior network\n",
    "    out_dim = clip_emb_dim\n",
    "    depth = 6\n",
    "    dim_head = 52\n",
    "    heads = clip_emb_dim//52 # heads * dim_head = clip_emb_dim\n",
    "    timesteps = 100\n",
    "\n",
    "    prior_network = PriorNetwork(\n",
    "            dim=out_dim,\n",
    "            depth=depth,\n",
    "            dim_head=dim_head,\n",
    "            heads=heads,\n",
    "            causal=False,\n",
    "            num_tokens = clip_seq_dim,\n",
    "            learned_query_mode=\"pos_emb\"\n",
    "        )\n",
    "\n",
    "    model.diffusion_prior = BrainDiffusionPrior(\n",
    "        net=prior_network,\n",
    "        image_embed_dim=out_dim,\n",
    "        condition_on_text_encodings=False,\n",
    "        timesteps=timesteps,\n",
    "        cond_drop_prob=0.2,\n",
    "        image_embed_scale=None,\n",
    "    )\n",
    "    \n",
    "    utils.count_params(model.diffusion_prior)\n",
    "    utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6c8e6-5c12-49b3-99b3-69525794f7a6",
   "metadata": {},
   "source": [
    "## Transfer from fMRI to EEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d160927-eb31-4a75-9e89-02a4cdced241",
   "metadata": {},
   "source": [
    "### Check eeg input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4eca2bf5-fc86-4159-9c8b-ecaa51c00ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG shape  (16, 63, 100)\n",
      "eeg_input_dim  6300\n",
      "hidden_dim 1024\n"
     ]
    }
   ],
   "source": [
    "eeg_example = next(iter(train_dls[0]))\n",
    "eeg_example = np.array(eeg_example[0])\n",
    "print(\"EEG shape \", eeg_example.shape)\n",
    "eeg_input_dim = eeg_example[0].flatten().shape[0]\n",
    "print(\"eeg_input_dim \", eeg_input_dim)\n",
    "print(\"hidden_dim\", hidden_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132829a7",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "038a5d61-4769-40b9-a004-f4e7b5b38bb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #For EEG training with latent space preinitialized by fMRI based model\n",
    "num_voxels_list = [ 15724, 15226, 13153, 13039, 17907, 12682, 14386, 15724, 15724, 15724] # fmri voxels dimentions for original MindEyeV2 Model\n",
    "class RidgeRegression(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_sizes, out_features): \n",
    "        super(RidgeRegression, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([\n",
    "                torch.nn.Linear(input_size, out_features) for input_size in input_sizes\n",
    "            ])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "        \n",
    "\n",
    "# # For EEG training without preinitialized latent space\n",
    "class RidgeRegressionEEG(torch.nn.Module):\n",
    "    # make sure to add weight_decay when initializing optimizer to enable regularization\n",
    "    def __init__(self, input_size, out_features, subj_list): \n",
    "        super(RidgeRegressionEEG, self).__init__()\n",
    "        self.out_features = out_features\n",
    "        self.linears = torch.nn.ModuleList([ torch.nn.Linear(input_size, out_features) for _ in subj_list])\n",
    "    def forward(self, x, subj_idx):\n",
    "        out = self.linears[subj_idx](x[:,0]).unsqueeze(1)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c2b09f",
   "metadata": {},
   "source": [
    "### Load checkpoint\n",
    "If fmri model checkpoint is specified replace original Ridge Regression (for fMRI input) with Ridge for EEG input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7782ab74-25ec-4b8a-8ace-222c8af332f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ckpt(tag,load_lr=True,load_optimizer=True,load_epoch=True,strict=True,outdir=outdir,multisubj_loading=False): \n",
    "    print(f\"\\n---loading {outdir}/{tag}.pth ckpt---\\n\")\n",
    "    checkpoint = torch.load(outdir+'/last.pth', map_location='cpu')\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "    if multisubj_loading: # remove incompatible ridge layer that will otherwise error\n",
    "        state_dict.pop('ridge.linears.0.weight',None)\n",
    "    model.load_state_dict(state_dict, strict=strict)\n",
    "    if load_epoch:\n",
    "        globals()[\"epoch\"] = checkpoint['epoch']\n",
    "        print(\"Epoch\",epoch)\n",
    "    if load_optimizer:\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    if load_lr:\n",
    "        lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "    del checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a7c7b-fe5e-41a4-80bf-d2814b3a57cc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "param counts:\n",
      "16,102,400 total\n",
      "16,102,400 trainable\n",
      "param counts:\n",
      "734,852,732 total\n",
      "734,852,716 trainable\n",
      "torch.Size([2, 1, 15724]) torch.Size([2, 1, 1024])\n",
      "\n",
      "---loading /net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/train_logs/multisubject_subj01_1024hid_nolow_300ep/last.pth ckpt---\n",
      "\n",
      "[2024-12-09 01:55:27,770] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "param counts:\n",
      "6,452,224 total\n",
      "6,452,224 trainable\n",
      "param counts:\n",
      "725,202,556 total\n",
      "725,202,540 trainable\n",
      "torch.Size([2, 1, 6300]) torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "# load multisubject stage1 ckpt if set\n",
    "if multisubject_ckpt is not None:\n",
    "    model.ridge = RidgeRegression(num_voxels_list, out_features=1024)\n",
    "    utils.count_params(model.ridge)\n",
    "    utils.count_params(model)\n",
    "    \n",
    "    load_ckpt(\"last\",outdir=multisubject_ckpt,load_lr=False,load_optimizer=False,load_epoch=False,strict=False,multisubj_loading=True)\n",
    "\n",
    "    \n",
    "# Replace with new trainable ridge in case of checkpoint\n",
    "# Create new model component in case of no checkpoint\n",
    "model.ridge = RidgeRegressionEEG(eeg_input_dim, out_features=hidden_dim, subj_list=subj_list)\n",
    "utils.count_params(model.ridge)\n",
    "utils.count_params(model)\n",
    "b = torch.randn((2,1,eeg_input_dim))\n",
    "print(b.shape, model.ridge(b,0).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec25271a-2209-400c-8026-df3b8ddc1eef",
   "metadata": {},
   "source": [
    "### Setup optimizer / lr / ckpt saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aefe7c27-ab39-4b2c-90f4-480f4087b7ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size = 16 num_iterations_per_epoch = 1033 num_samples_per_epoch = 16540\n"
     ]
    }
   ],
   "source": [
    "num_samples_per_epoch = num_train // num_devices \n",
    "\n",
    "num_iterations_per_epoch = num_samples_per_epoch // (batch_size)\n",
    "\n",
    "print(\"batch_size =\", batch_size, \"num_iterations_per_epoch =\",num_iterations_per_epoch, \"num_samples_per_epoch =\",num_samples_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e14d0482-dc42-43b9-9ce1-953c32f2c9c1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_steps 2066\n",
      "\n",
      "Done with model preparations!\n",
      "param counts:\n",
      "725,202,556 total\n",
      "725,202,540 trainable\n"
     ]
    }
   ],
   "source": [
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "\n",
    "opt_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.ridge.named_parameters()], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "    {'params': [p for n, p in model.backbone.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "]\n",
    "if use_prior:\n",
    "    opt_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 1e-2},\n",
    "        {'params': [p for n, p in model.diffusion_prior.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ])\n",
    "\n",
    "optimizer = torch.optim.AdamW(opt_grouped_parameters, lr=max_lr)\n",
    "\n",
    "if lr_scheduler_type == 'linear':\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
    "        optimizer,\n",
    "        total_iters=int(np.floor(num_epochs*num_iterations_per_epoch)),\n",
    "        last_epoch=-1\n",
    "    )\n",
    "elif lr_scheduler_type == 'cycle':\n",
    "    total_steps=int(np.floor(num_epochs*num_iterations_per_epoch))\n",
    "    print(\"total_steps\", total_steps)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer, \n",
    "        max_lr=max_lr,\n",
    "        total_steps=total_steps,\n",
    "        final_div_factor=1000,\n",
    "        last_epoch=-1, pct_start=2/num_epochs\n",
    "    )\n",
    "    \n",
    "def save_ckpt(tag):\n",
    "    ckpt_path = outdir+f'/{tag}.pth'\n",
    "    if accelerator.is_main_process:\n",
    "        unwrapped_model = accelerator.unwrap_model(model)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': unwrapped_model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'lr_scheduler': lr_scheduler.state_dict(),\n",
    "            'train_losses': losses,\n",
    "            'test_losses': test_losses,\n",
    "            'lrs': lrs,\n",
    "            }, ckpt_path)\n",
    "    print(f\"\\n---saved {outdir}/{tag} ckpt!---\\n\")\n",
    "\n",
    "\n",
    "print(\"\\nDone with model preparations!\")\n",
    "num_params = utils.count_params(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983f458b-35b8-49f2-b6db-80296cece730",
   "metadata": {},
   "source": [
    "# Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a25a662-daa8-4de9-9233-8364800fcb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if local_rank==0 and wandb_log: # only use main process for wandb logging\n",
    "    import wandb\n",
    "    wandb_project = 'mindeye'\n",
    "    print(f\"wandb {wandb_project} run {model_name}\")\n",
    "    # need to configure wandb beforehand in terminal with \"wandb init\"!\n",
    "    wandb_config = {\n",
    "      \"model_name\": model_name,\n",
    "      \"global_batch_size\": global_batch_size,\n",
    "      \"batch_size\": batch_size,\n",
    "      \"num_epochs\": num_epochs,\n",
    "      \"num_params\": num_params,\n",
    "      \"clip_scale\": clip_scale,\n",
    "      \"prior_scale\": prior_scale,\n",
    "      \"blur_scale\": blur_scale,\n",
    "      \"use_image_aug\": use_image_aug,\n",
    "      \"max_lr\": max_lr,\n",
    "      \"mixup_pct\": mixup_pct,\n",
    "      \"num_samples_per_epoch\": num_samples_per_epoch,\n",
    "      \"num_test\": num_test,\n",
    "      \"num_train\": num_train,\n",
    "      \"ckpt_interval\": ckpt_interval,\n",
    "      \"ckpt_saving\": ckpt_saving,\n",
    "      \"seed\": seed,\n",
    "      \"distributed\": distributed,\n",
    "      \"num_devices\": num_devices,\n",
    "      \"world_size\": world_size,\n",
    "      \"preprocessing_method\": preprocessing_method\n",
    "    }\n",
    "    print(\"wandb_config:\\n\",wandb_config)\n",
    "    print(\"wandb_id:\",model_name)\n",
    "    wandb.init(\n",
    "        id=model_name,\n",
    "        project=wandb_project,\n",
    "        name=model_name,\n",
    "        config=wandb_config,\n",
    "        resume=\"allow\",\n",
    "    )\n",
    "else:\n",
    "    wandb_log = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5690151-2131-4918-b750-e869cbd1a8a8",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12de6387-6e18-4e4b-b5ce-a847d625330a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "losses, test_losses, lrs = [], [], []\n",
    "best_test_loss = 1e9\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "99f09f76-4481-4133-b09a-a22b10dbc0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model, optimizer, *train_dls, lr_scheduler = accelerator.prepare(model, optimizer, *train_dls, lr_scheduler)\n",
    "# leaving out test_dl since we will only have local_rank 0 device do evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c7d1037b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MindEyeModule(\n",
      "  (backbone): BrainNetwork(\n",
      "    (mixer_blocks1): ModuleList(\n",
      "      (0-3): 4 x Sequential(\n",
      "        (0): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.15, inplace=False)\n",
      "          (3): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (mixer_blocks2): ModuleList(\n",
      "      (0-3): 4 x Sequential(\n",
      "        (0): LayerNorm((1,), eps=1e-05, elementwise_affine=True)\n",
      "        (1): Sequential(\n",
      "          (0): Linear(in_features=1, out_features=1, bias=True)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Dropout(p=0.15, inplace=False)\n",
      "          (3): Linear(in_features=1, out_features=1, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (backbone_linear): Linear(in_features=1024, out_features=425984, bias=True)\n",
      "    (clip_proj): Sequential(\n",
      "      (0): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "      (1): GELU(approximate='none')\n",
      "      (2): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "      (3): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "      (4): GELU(approximate='none')\n",
      "      (5): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "      (6): LayerNorm((1664,), eps=1e-05, elementwise_affine=True)\n",
      "      (7): GELU(approximate='none')\n",
      "      (8): Linear(in_features=1664, out_features=1664, bias=True)\n",
      "    )\n",
      "    (blin1): Linear(in_features=1024, out_features=3136, bias=True)\n",
      "    (bdropout): Dropout(p=0.3, inplace=False)\n",
      "    (bnorm): GroupNorm(1, 64, eps=1e-05, affine=True)\n",
      "    (bupsampler): Decoder(\n",
      "      (conv_in): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (up_blocks): ModuleList(\n",
      "        (0): UpDecoderBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0-1): 2 x ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): UpDecoderBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "              (conv1): LoRACompatibleConv(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): LoRACompatibleConv(128, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "              (conv1): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "          (upsamplers): ModuleList(\n",
      "            (0): Upsample2D(\n",
      "              (conv): LoRACompatibleConv(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): UpDecoderBlock2D(\n",
      "          (resnets): ModuleList(\n",
      "            (0): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 64, eps=1e-06, affine=True)\n",
      "              (conv1): LoRACompatibleConv(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): LoRACompatibleConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "              (conv_shortcut): LoRACompatibleConv(64, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "            )\n",
      "            (1): ResnetBlock2D(\n",
      "              (norm1): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
      "              (conv1): LoRACompatibleConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (norm2): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (conv2): LoRACompatibleConv(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "              (nonlinearity): SiLU()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (mid_block): UNetMidBlock2D(\n",
      "        (attentions): ModuleList(\n",
      "          (0): Attention(\n",
      "            (group_norm): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (to_q): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
      "            (to_k): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
      "            (to_v): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
      "            (to_out): ModuleList(\n",
      "              (0): LoRACompatibleLinear(in_features=128, out_features=128, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (resnets): ModuleList(\n",
      "          (0-1): 2 x ResnetBlock2D(\n",
      "            (norm1): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (conv1): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (norm2): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (conv2): LoRACompatibleConv(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "            (nonlinearity): SiLU()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (conv_norm_out): GroupNorm(32, 32, eps=1e-06, affine=True)\n",
      "      (conv_act): SiLU()\n",
      "      (conv_out): Conv2d(32, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (b_maps_projector): Sequential(\n",
      "      (0): Conv2d(64, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (4): GroupNorm(1, 512, eps=1e-05, affine=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "      (6): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (diffusion_prior): BrainDiffusionPrior(\n",
      "    (noise_scheduler): NoiseScheduler()\n",
      "    (net): PriorNetwork(\n",
      "      (to_time_embeds): Sequential(\n",
      "        (0): Sequential(\n",
      "          (0): SinusoidalPosEmb()\n",
      "          (1): MLP(\n",
      "            (net): Sequential(\n",
      "              (0): Sequential(\n",
      "                (0): Linear(in_features=1664, out_features=3328, bias=True)\n",
      "                (1): SiLU()\n",
      "                (2): Identity()\n",
      "              )\n",
      "              (1): Sequential(\n",
      "                (0): Linear(in_features=3328, out_features=3328, bias=True)\n",
      "                (1): SiLU()\n",
      "                (2): Identity()\n",
      "              )\n",
      "              (2): Linear(in_features=3328, out_features=1664, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): Rearrange('b (n d) -> b n d', n=1)\n",
      "      )\n",
      "      (causal_transformer): FlaggedCausalTransformer(\n",
      "        (init_norm): Identity()\n",
      "        (rel_pos_bias): RelPosBias(\n",
      "          (relative_attention_bias): Embedding(32, 32)\n",
      "        )\n",
      "        (layers): ModuleList(\n",
      "          (0-5): 6 x ModuleList(\n",
      "            (0): Attention(\n",
      "              (norm): LayerNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "              (to_q): Linear(in_features=1664, out_features=1664, bias=False)\n",
      "              (to_kv): Linear(in_features=1664, out_features=104, bias=False)\n",
      "              (rotary_emb): RotaryEmbedding()\n",
      "              (to_out): Sequential(\n",
      "                (0): Linear(in_features=1664, out_features=1664, bias=False)\n",
      "                (1): LayerNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): Sequential(\n",
      "              (0): LayerNorm()\n",
      "              (1): Linear(in_features=1664, out_features=13312, bias=False)\n",
      "              (2): SwiGLU()\n",
      "              (3): Identity()\n",
      "              (4): Dropout(p=0.0, inplace=False)\n",
      "              (5): Linear(in_features=6656, out_features=1664, bias=False)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (norm): LayerNorm()\n",
      "        (project_out): Linear(in_features=1664, out_features=1664, bias=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ridge): RidgeRegressionEEG(\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=6300, out_features=1024, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfe6c0e-ac28-4fb4-973a-ee8112a94a20",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e88f5178-b7c3-4842-8e60-fb8138ddebac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Number of Epochs planned: \", num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60be0d5f-3e94-4612-9373-61b53d836393",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eeg_multisubject_fmri_init starting with epoch 0 / 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 0/2 [00:00<?, ?it/s]\n",
      "  0%|          | 0/1033 [00:00<?, ?it/s]\u001b[A\n",
      "  0%|          | 1/1033 [00:00<02:33,  6.71it/s]\u001b[A\n",
      "  0%|          | 2/1033 [00:00<02:43,  6.31it/s]\u001b[A\n",
      "  0%|          | 4/1033 [00:00<01:44,  9.81it/s]\u001b[A\n",
      "  1%|          | 6/1033 [00:00<01:27, 11.68it/s]\u001b[A\n",
      "  1%|          | 8/1033 [00:00<01:20, 12.72it/s]\u001b[A\n",
      "  1%|          | 10/1033 [00:00<01:16, 13.40it/s]\u001b[A\n",
      "  1%|          | 12/1033 [00:00<01:14, 13.79it/s]\u001b[A\n",
      "  1%|         | 14/1033 [00:01<01:12, 14.11it/s]\u001b[A\n",
      "  2%|         | 16/1033 [00:01<01:11, 14.22it/s]\u001b[A\n",
      "  2%|         | 18/1033 [00:01<01:11, 14.25it/s]\u001b[A\n",
      "  2%|         | 20/1033 [00:01<01:22, 12.28it/s]\u001b[A\n",
      "  2%|         | 22/1033 [00:01<01:18, 12.85it/s]\u001b[A\n",
      "  2%|         | 24/1033 [00:01<01:15, 13.38it/s]\u001b[A\n",
      "  3%|         | 26/1033 [00:02<01:15, 13.29it/s]\u001b[A\n",
      "  3%|         | 28/1033 [00:02<01:13, 13.60it/s]\u001b[A\n",
      "  3%|         | 30/1033 [00:02<01:11, 13.98it/s]\u001b[A\n",
      "  3%|         | 32/1033 [00:02<01:09, 14.38it/s]\u001b[A\n",
      "  3%|         | 34/1033 [00:02<01:09, 14.35it/s]\u001b[A\n",
      "  3%|         | 36/1033 [00:02<01:08, 14.61it/s]\u001b[A\n",
      "  4%|         | 38/1033 [00:02<01:17, 12.79it/s]\u001b[A\n",
      "  4%|         | 40/1033 [00:03<01:14, 13.38it/s]\u001b[A\n",
      "  4%|         | 42/1033 [00:03<01:13, 13.57it/s]\u001b[A\n",
      "  4%|         | 44/1033 [00:03<01:13, 13.42it/s]\u001b[A\n",
      "  4%|         | 46/1033 [00:03<01:11, 13.77it/s]\u001b[A\n",
      "  5%|         | 48/1033 [00:03<01:10, 14.05it/s]\u001b[A\n",
      "  5%|         | 50/1033 [00:03<01:09, 14.17it/s]\u001b[A\n",
      "  5%|         | 52/1033 [00:03<01:08, 14.23it/s]\u001b[A\n",
      "  5%|         | 54/1033 [00:04<01:08, 14.39it/s]\u001b[A\n",
      "  5%|         | 56/1033 [00:04<01:17, 12.66it/s]\u001b[A\n",
      "  6%|         | 58/1033 [00:04<01:13, 13.21it/s]\u001b[A\n",
      "  6%|         | 60/1033 [00:04<01:10, 13.72it/s]\u001b[A\n",
      "  6%|         | 62/1033 [00:04<01:09, 13.98it/s]\u001b[A\n",
      "  6%|         | 64/1033 [00:04<01:06, 14.46it/s]\u001b[A\n",
      "  6%|         | 66/1033 [00:04<01:06, 14.65it/s]\u001b[A\n",
      "  7%|         | 68/1033 [00:05<01:04, 15.01it/s]\u001b[A\n",
      "  7%|         | 70/1033 [00:05<01:04, 15.00it/s]\u001b[A\n",
      "  7%|         | 72/1033 [00:05<01:05, 14.71it/s]\u001b[A\n",
      "  7%|         | 74/1033 [00:05<01:04, 14.79it/s]\u001b[A\n",
      "  7%|         | 76/1033 [00:05<01:14, 12.77it/s]\u001b[A\n",
      "  8%|         | 78/1033 [00:05<01:12, 13.25it/s]\u001b[A\n",
      "  8%|         | 80/1033 [00:05<01:09, 13.72it/s]\u001b[A\n",
      "  8%|         | 82/1033 [00:06<01:08, 13.91it/s]\u001b[A\n",
      "  8%|         | 84/1033 [00:06<01:06, 14.37it/s]\u001b[A\n",
      "  8%|         | 86/1033 [00:06<01:05, 14.42it/s]\u001b[A\n",
      "  9%|         | 88/1033 [00:06<01:04, 14.55it/s]\u001b[A\n",
      "  9%|         | 90/1033 [00:06<01:05, 14.47it/s]\u001b[A\n",
      "  9%|         | 92/1033 [00:06<01:05, 14.45it/s]\u001b[A\n",
      "  9%|         | 94/1033 [00:06<01:14, 12.63it/s]\u001b[A\n",
      "  9%|         | 96/1033 [00:07<01:10, 13.36it/s]\u001b[A\n",
      "  9%|         | 98/1033 [00:07<01:07, 13.77it/s]\u001b[A\n",
      " 10%|         | 100/1033 [00:07<01:05, 14.21it/s]\u001b[A\n",
      " 10%|         | 102/1033 [00:07<01:04, 14.46it/s]\u001b[A\n",
      " 10%|         | 104/1033 [00:07<01:03, 14.57it/s]\u001b[A\n",
      " 10%|         | 106/1033 [00:07<01:03, 14.56it/s]\u001b[A\n",
      " 10%|         | 108/1033 [00:07<01:02, 14.75it/s]\u001b[A\n",
      " 11%|         | 110/1033 [00:08<01:02, 14.72it/s]\u001b[A\n",
      " 11%|         | 112/1033 [00:08<01:06, 13.78it/s]\u001b[A\n",
      " 11%|         | 114/1033 [00:08<01:10, 13.07it/s]\u001b[A\n",
      " 11%|         | 116/1033 [00:08<01:07, 13.65it/s]\u001b[A\n",
      " 11%|        | 118/1033 [00:08<01:05, 14.04it/s]\u001b[A\n",
      " 12%|        | 120/1033 [00:08<01:03, 14.38it/s]\u001b[A\n",
      " 12%|        | 122/1033 [00:08<01:02, 14.48it/s]\u001b[A\n",
      " 12%|        | 124/1033 [00:09<01:02, 14.63it/s]\u001b[A\n",
      " 12%|        | 126/1033 [00:09<01:01, 14.84it/s]\u001b[A\n",
      " 12%|        | 128/1033 [00:09<01:02, 14.43it/s]\u001b[A\n",
      " 13%|        | 130/1033 [00:09<01:01, 14.62it/s]\u001b[A\n",
      " 13%|        | 132/1033 [00:09<01:10, 12.73it/s]\u001b[A\n",
      " 13%|        | 134/1033 [00:09<01:07, 13.34it/s]\u001b[A\n",
      " 13%|        | 136/1033 [00:09<01:04, 13.81it/s]\u001b[A\n",
      " 13%|        | 138/1033 [00:10<01:03, 14.17it/s]\u001b[A\n",
      " 14%|        | 140/1033 [00:10<01:02, 14.37it/s]\u001b[A\n",
      " 14%|        | 142/1033 [00:10<01:01, 14.60it/s]\u001b[A\n",
      " 14%|        | 144/1033 [00:10<01:02, 14.20it/s]\u001b[A\n",
      " 14%|        | 146/1033 [00:10<01:01, 14.42it/s]\u001b[A\n",
      " 14%|        | 148/1033 [00:10<01:00, 14.61it/s]\u001b[A\n",
      " 15%|        | 150/1033 [00:10<01:09, 12.70it/s]\u001b[A\n",
      " 15%|        | 152/1033 [00:11<01:05, 13.43it/s]\u001b[A\n",
      " 15%|        | 154/1033 [00:11<01:03, 13.78it/s]\u001b[A\n",
      " 15%|        | 156/1033 [00:11<01:01, 14.23it/s]\u001b[A\n",
      " 15%|        | 158/1033 [00:11<01:00, 14.50it/s]\u001b[A\n",
      " 15%|        | 160/1033 [00:11<01:00, 14.40it/s]\u001b[A\n",
      " 16%|        | 162/1033 [00:11<01:00, 14.49it/s]\u001b[A\n",
      " 16%|        | 164/1033 [00:11<00:59, 14.58it/s]\u001b[A\n",
      " 16%|        | 166/1033 [00:11<01:00, 14.45it/s]\u001b[A\n",
      " 16%|        | 168/1033 [00:12<00:59, 14.42it/s]\u001b[A\n",
      " 16%|        | 170/1033 [00:12<01:08, 12.66it/s]\u001b[A\n",
      " 17%|        | 172/1033 [00:12<01:04, 13.27it/s]\u001b[A\n",
      " 17%|        | 174/1033 [00:12<01:02, 13.70it/s]\u001b[A\n",
      " 17%|        | 176/1033 [00:12<01:00, 14.21it/s]\u001b[A\n",
      " 17%|        | 178/1033 [00:12<00:59, 14.48it/s]\u001b[A\n",
      " 17%|        | 180/1033 [00:13<00:58, 14.52it/s]\u001b[A\n",
      " 18%|        | 182/1033 [00:13<00:57, 14.77it/s]\u001b[A\n",
      " 18%|        | 184/1033 [00:13<00:57, 14.64it/s]\u001b[A\n",
      " 18%|        | 186/1033 [00:13<01:04, 13.18it/s]\u001b[A\n",
      " 18%|        | 188/1033 [00:13<01:10, 11.92it/s]\u001b[A\n",
      " 18%|        | 190/1033 [00:13<01:06, 12.72it/s]\u001b[A\n",
      " 19%|        | 192/1033 [00:13<01:03, 13.26it/s]\u001b[A\n",
      " 19%|        | 194/1033 [00:14<01:01, 13.68it/s]\u001b[A\n",
      " 19%|        | 196/1033 [00:14<00:59, 14.08it/s]\u001b[A\n",
      " 19%|        | 198/1033 [00:14<00:58, 14.26it/s]\u001b[A\n",
      " 19%|        | 200/1033 [00:14<00:57, 14.45it/s]\u001b[A\n",
      " 20%|        | 202/1033 [00:14<00:56, 14.71it/s]\u001b[A\n",
      " 20%|        | 204/1033 [00:14<00:55, 14.88it/s]\u001b[A\n",
      " 20%|        | 206/1033 [00:14<01:04, 12.91it/s]\u001b[A\n",
      " 20%|        | 208/1033 [00:15<01:01, 13.40it/s]\u001b[A\n",
      " 20%|        | 210/1033 [00:15<00:59, 13.81it/s]\u001b[A\n",
      " 21%|        | 212/1033 [00:15<00:57, 14.22it/s]\u001b[A\n",
      " 21%|        | 214/1033 [00:15<00:56, 14.47it/s]\u001b[A\n",
      " 21%|        | 216/1033 [00:15<00:55, 14.77it/s]\u001b[A\n",
      " 21%|        | 218/1033 [00:15<00:54, 14.96it/s]\u001b[A\n",
      " 21%|       | 220/1033 [00:15<00:54, 14.82it/s]\u001b[A\n",
      " 21%|       | 222/1033 [00:15<00:54, 14.95it/s]\u001b[A\n",
      " 22%|       | 224/1033 [00:16<00:54, 14.88it/s]\u001b[A\n",
      " 22%|       | 226/1033 [00:16<01:02, 12.92it/s]\u001b[A\n",
      " 22%|       | 228/1033 [00:16<01:00, 13.41it/s]\u001b[A\n",
      " 22%|       | 230/1033 [00:16<00:58, 13.79it/s]\u001b[A\n",
      " 22%|       | 232/1033 [00:16<00:56, 14.09it/s]\u001b[A\n",
      " 23%|       | 234/1033 [00:16<00:55, 14.30it/s]\u001b[A\n",
      " 23%|       | 236/1033 [00:17<00:59, 13.49it/s]\u001b[A\n",
      " 23%|       | 238/1033 [00:17<00:56, 13.99it/s]\u001b[A\n",
      " 23%|       | 240/1033 [00:17<00:56, 14.05it/s]\u001b[A\n",
      " 23%|       | 242/1033 [00:17<00:55, 14.28it/s]\u001b[A\n",
      " 24%|       | 244/1033 [00:17<01:02, 12.70it/s]\u001b[A\n",
      " 24%|       | 246/1033 [00:17<00:59, 13.33it/s]\u001b[A\n",
      " 24%|       | 248/1033 [00:17<00:57, 13.64it/s]\u001b[A\n",
      " 24%|       | 250/1033 [00:18<00:56, 13.90it/s]\u001b[A\n",
      " 24%|       | 252/1033 [00:18<00:55, 14.10it/s]\u001b[A\n",
      " 25%|       | 254/1033 [00:18<00:54, 14.40it/s]\u001b[A\n",
      " 25%|       | 256/1033 [00:18<00:55, 13.92it/s]\u001b[A\n",
      " 25%|       | 258/1033 [00:18<00:54, 14.27it/s]\u001b[A\n",
      " 25%|       | 260/1033 [00:18<00:53, 14.53it/s]\u001b[A\n",
      " 25%|       | 262/1033 [00:18<01:00, 12.68it/s]\u001b[A\n",
      " 26%|       | 264/1033 [00:19<00:57, 13.29it/s]\u001b[A\n",
      " 26%|       | 266/1033 [00:19<00:56, 13.62it/s]\u001b[A\n",
      " 26%|       | 268/1033 [00:19<00:54, 14.00it/s]\u001b[A\n",
      " 26%|       | 270/1033 [00:19<00:53, 14.27it/s]\u001b[A\n",
      " 26%|       | 272/1033 [00:19<00:52, 14.45it/s]\u001b[A\n",
      " 27%|       | 274/1033 [00:19<00:51, 14.65it/s]\u001b[A\n",
      " 27%|       | 276/1033 [00:19<00:51, 14.69it/s]\u001b[A\n",
      " 27%|       | 278/1033 [00:20<00:51, 14.70it/s]\u001b[A\n",
      " 27%|       | 280/1033 [00:20<00:58, 12.90it/s]\u001b[A\n",
      " 27%|       | 282/1033 [00:20<00:55, 13.57it/s]\u001b[A\n",
      " 27%|       | 284/1033 [00:20<00:54, 13.72it/s]\u001b[A\n",
      " 28%|       | 286/1033 [00:20<00:53, 14.02it/s]\u001b[A\n",
      " 28%|       | 288/1033 [00:20<00:53, 14.03it/s]\u001b[A\n",
      " 28%|       | 290/1033 [00:20<00:52, 14.20it/s]\u001b[A\n",
      " 28%|       | 292/1033 [00:21<00:51, 14.40it/s]\u001b[A\n",
      " 28%|       | 294/1033 [00:21<00:51, 14.36it/s]\u001b[A\n",
      " 29%|       | 296/1033 [00:21<00:50, 14.55it/s]\u001b[A\n",
      " 29%|       | 298/1033 [00:21<00:50, 14.55it/s]\u001b[A\n",
      " 29%|       | 300/1033 [00:21<00:58, 12.59it/s]\u001b[A\n",
      " 29%|       | 302/1033 [00:21<00:55, 13.09it/s]\u001b[A\n",
      " 29%|       | 304/1033 [00:21<00:54, 13.44it/s]\u001b[A\n",
      " 30%|       | 306/1033 [00:22<00:52, 13.95it/s]\u001b[A\n",
      " 30%|       | 308/1033 [00:22<00:53, 13.58it/s]\u001b[A\n",
      " 30%|       | 310/1033 [00:22<00:52, 13.79it/s]\u001b[A\n",
      " 30%|       | 312/1033 [00:22<00:51, 14.04it/s]\u001b[A\n",
      " 30%|       | 314/1033 [00:22<00:50, 14.29it/s]\u001b[A\n",
      " 31%|       | 316/1033 [00:22<00:49, 14.46it/s]\u001b[A\n",
      " 31%|       | 318/1033 [00:22<00:56, 12.59it/s]\u001b[A\n",
      " 31%|       | 320/1033 [00:23<00:54, 13.13it/s]\u001b[A\n",
      " 31%|       | 322/1033 [00:23<00:52, 13.56it/s]\u001b[A\n",
      " 31%|      | 324/1033 [00:23<00:51, 13.80it/s]\u001b[A\n",
      " 32%|      | 326/1033 [00:23<00:50, 14.09it/s]\u001b[A\n",
      " 32%|      | 328/1033 [00:23<00:49, 14.27it/s]\u001b[A\n",
      " 32%|      | 330/1033 [00:23<00:47, 14.65it/s]\u001b[A\n",
      " 32%|      | 332/1033 [00:23<00:47, 14.72it/s]\u001b[A\n",
      " 32%|      | 334/1033 [00:24<00:47, 14.68it/s]\u001b[A\n",
      " 33%|      | 336/1033 [00:24<00:54, 12.89it/s]\u001b[A\n",
      " 33%|      | 338/1033 [00:24<00:52, 13.33it/s]\u001b[A\n",
      " 33%|      | 340/1033 [00:24<00:54, 12.82it/s]\u001b[A\n",
      " 33%|      | 342/1033 [00:24<00:51, 13.36it/s]\u001b[A\n",
      " 33%|      | 344/1033 [00:24<00:49, 13.82it/s]\u001b[A\n",
      " 33%|      | 346/1033 [00:24<00:47, 14.40it/s]\u001b[A\n",
      " 34%|      | 348/1033 [00:25<00:47, 14.39it/s]\u001b[A\n",
      " 34%|      | 350/1033 [00:25<00:46, 14.70it/s]\u001b[A\n",
      " 34%|      | 352/1033 [00:25<00:45, 14.82it/s]\u001b[A\n",
      " 34%|      | 354/1033 [00:25<00:52, 12.95it/s]\u001b[A\n",
      " 34%|      | 356/1033 [00:25<00:50, 13.43it/s]\u001b[A\n",
      " 35%|      | 358/1033 [00:25<00:48, 13.96it/s]\u001b[A\n",
      " 35%|      | 360/1033 [00:25<00:47, 14.30it/s]\u001b[A\n",
      " 35%|      | 362/1033 [00:26<00:46, 14.40it/s]\u001b[A\n",
      " 35%|      | 364/1033 [00:26<00:46, 14.46it/s]\u001b[A\n",
      " 35%|      | 366/1033 [00:26<00:45, 14.72it/s]\u001b[A\n",
      " 36%|      | 368/1033 [00:26<00:45, 14.76it/s]\u001b[A\n",
      " 36%|      | 370/1033 [00:26<00:44, 14.83it/s]\u001b[A\n",
      " 36%|      | 372/1033 [00:26<00:44, 14.70it/s]\u001b[A\n",
      " 36%|      | 374/1033 [00:26<00:51, 12.73it/s]\u001b[A\n",
      " 36%|      | 376/1033 [00:27<00:51, 12.88it/s]\u001b[A\n",
      " 37%|      | 378/1033 [00:27<00:49, 13.26it/s]\u001b[A\n",
      " 37%|      | 380/1033 [00:27<00:49, 13.14it/s]\u001b[A\n",
      " 37%|      | 382/1033 [00:27<00:48, 13.55it/s]\u001b[A\n",
      " 37%|      | 384/1033 [00:27<00:46, 13.87it/s]\u001b[A\n",
      " 37%|      | 386/1033 [00:27<00:45, 14.17it/s]\u001b[A\n",
      " 38%|      | 388/1033 [00:27<00:44, 14.50it/s]\u001b[A\n",
      " 38%|      | 390/1033 [00:28<00:43, 14.74it/s]\u001b[A\n",
      " 38%|      | 392/1033 [00:28<00:50, 12.73it/s]\u001b[A\n",
      " 38%|      | 394/1033 [00:28<00:48, 13.31it/s]\u001b[A\n",
      " 38%|      | 396/1033 [00:28<00:46, 13.63it/s]\u001b[A\n",
      " 39%|      | 398/1033 [00:28<00:45, 14.05it/s]\u001b[A\n",
      " 39%|      | 400/1033 [00:28<00:44, 14.31it/s]\u001b[A\n",
      " 39%|      | 402/1033 [00:28<00:43, 14.50it/s]\u001b[A\n",
      " 39%|      | 404/1033 [00:29<00:42, 14.63it/s]\u001b[A\n",
      " 39%|      | 406/1033 [00:29<00:42, 14.61it/s]\u001b[A\n",
      " 39%|      | 408/1033 [00:29<00:42, 14.75it/s]\u001b[A\n",
      " 40%|      | 410/1033 [00:29<00:49, 12.65it/s]\u001b[A\n",
      " 40%|      | 412/1033 [00:29<00:45, 13.56it/s]\u001b[A\n",
      " 40%|      | 414/1033 [00:29<00:45, 13.49it/s]\u001b[A\n",
      " 40%|      | 416/1033 [00:29<00:44, 13.85it/s]\u001b[A\n",
      " 40%|      | 418/1033 [00:30<00:43, 14.27it/s]\u001b[A\n",
      " 41%|      | 420/1033 [00:30<00:41, 14.60it/s]\u001b[A\n",
      " 41%|      | 422/1033 [00:30<00:41, 14.88it/s]\u001b[A\n",
      " 41%|      | 424/1033 [00:30<00:40, 15.00it/s]\u001b[A\n",
      " 41%|      | 426/1033 [00:30<00:40, 15.01it/s]\u001b[A\n",
      " 41%|     | 428/1033 [00:30<00:40, 14.92it/s]\u001b[A\n",
      " 42%|     | 430/1033 [00:30<00:46, 12.87it/s]\u001b[A\n",
      " 42%|     | 432/1033 [00:31<00:44, 13.41it/s]\u001b[A\n",
      " 42%|     | 434/1033 [00:31<00:43, 13.74it/s]\u001b[A\n",
      " 42%|     | 436/1033 [00:31<00:42, 14.13it/s]\u001b[A\n",
      " 42%|     | 438/1033 [00:31<00:41, 14.44it/s]\u001b[A\n",
      " 43%|     | 440/1033 [00:31<00:40, 14.61it/s]\u001b[A\n",
      " 43%|     | 442/1033 [00:31<00:39, 14.86it/s]\u001b[A\n",
      " 43%|     | 444/1033 [00:31<00:39, 14.84it/s]\u001b[A\n",
      " 43%|     | 446/1033 [00:32<00:39, 14.83it/s]\u001b[A\n",
      " 43%|     | 448/1033 [00:32<00:45, 12.87it/s]\u001b[A\n",
      " 44%|     | 450/1033 [00:32<00:43, 13.39it/s]\u001b[A\n",
      " 44%|     | 452/1033 [00:32<00:41, 13.90it/s]\u001b[A\n",
      " 44%|     | 454/1033 [00:32<00:40, 14.24it/s]\u001b[A\n",
      " 44%|     | 456/1033 [00:32<00:40, 14.35it/s]\u001b[A\n",
      " 44%|     | 458/1033 [00:32<00:39, 14.56it/s]\u001b[A\n",
      " 45%|     | 460/1033 [00:33<00:39, 14.52it/s]\u001b[A\n",
      " 45%|     | 462/1033 [00:33<00:38, 14.69it/s]\u001b[A\n",
      " 45%|     | 464/1033 [00:33<00:38, 14.80it/s]\u001b[A\n",
      " 45%|     | 466/1033 [00:33<00:41, 13.81it/s]\u001b[A\n",
      " 45%|     | 468/1033 [00:33<00:42, 13.34it/s]\u001b[A\n",
      " 45%|     | 470/1033 [00:33<00:41, 13.59it/s]\u001b[A\n",
      " 46%|     | 472/1033 [00:33<00:40, 13.93it/s]\u001b[A\n",
      " 46%|     | 474/1033 [00:34<00:39, 14.30it/s]\u001b[A\n",
      " 46%|     | 476/1033 [00:34<00:38, 14.38it/s]\u001b[A\n",
      " 46%|     | 478/1033 [00:34<00:38, 14.48it/s]\u001b[A\n",
      " 46%|     | 480/1033 [00:34<00:39, 13.85it/s]\u001b[A\n",
      " 47%|     | 482/1033 [00:34<00:38, 14.29it/s]\u001b[A\n",
      " 47%|     | 484/1033 [00:34<00:37, 14.50it/s]\u001b[A\n",
      " 47%|     | 486/1033 [00:34<00:42, 12.74it/s]\u001b[A\n",
      " 47%|     | 488/1033 [00:35<00:40, 13.44it/s]\u001b[A\n",
      " 47%|     | 490/1033 [00:35<00:39, 13.78it/s]\u001b[A\n",
      " 48%|     | 492/1033 [00:35<00:38, 14.17it/s]\u001b[A\n",
      " 48%|     | 494/1033 [00:35<00:37, 14.41it/s]\u001b[A\n",
      " 48%|     | 496/1033 [00:35<00:36, 14.59it/s]\u001b[A\n",
      " 48%|     | 498/1033 [00:35<00:36, 14.57it/s]\u001b[A\n",
      " 48%|     | 500/1033 [00:35<00:36, 14.68it/s]\u001b[A\n",
      " 49%|     | 502/1033 [00:36<00:35, 15.01it/s]\u001b[A\n",
      " 49%|     | 504/1033 [00:36<00:41, 12.78it/s]\u001b[A\n",
      " 49%|     | 506/1033 [00:36<00:39, 13.27it/s]\u001b[A\n",
      " 49%|     | 508/1033 [00:36<00:37, 13.91it/s]\u001b[A\n",
      " 49%|     | 510/1033 [00:36<00:36, 14.27it/s]\u001b[A\n",
      " 50%|     | 512/1033 [00:36<00:35, 14.48it/s]\u001b[A\n",
      " 50%|     | 514/1033 [00:36<00:36, 14.31it/s]\u001b[A\n",
      " 50%|     | 516/1033 [00:37<00:35, 14.62it/s]\u001b[A\n",
      " 50%|     | 518/1033 [00:37<00:34, 14.88it/s]\u001b[A\n",
      " 50%|     | 520/1033 [00:37<00:34, 14.96it/s]\u001b[A\n",
      " 51%|     | 522/1033 [00:37<00:33, 15.06it/s]\u001b[A\n",
      " 51%|     | 524/1033 [00:37<00:39, 12.89it/s]\u001b[A\n",
      " 51%|     | 526/1033 [00:37<00:37, 13.53it/s]\u001b[A\n",
      " 51%|     | 528/1033 [00:37<00:36, 13.89it/s]\u001b[A\n",
      " 51%|    | 530/1033 [00:38<00:35, 14.31it/s]\u001b[A\n",
      " 52%|    | 532/1033 [00:38<00:34, 14.55it/s]\u001b[A\n",
      " 52%|    | 534/1033 [00:38<00:34, 14.63it/s]\u001b[A\n",
      " 52%|    | 536/1033 [00:38<00:34, 14.59it/s]\u001b[A\n",
      " 52%|    | 538/1033 [00:38<00:33, 14.70it/s]\u001b[A\n",
      " 52%|    | 540/1033 [00:38<00:33, 14.89it/s]\u001b[A\n",
      " 52%|    | 542/1033 [00:38<00:37, 13.03it/s]\u001b[A\n",
      " 53%|    | 544/1033 [00:39<00:35, 13.60it/s]\u001b[A\n",
      " 53%|    | 546/1033 [00:39<00:34, 14.02it/s]\u001b[A\n",
      " 53%|    | 548/1033 [00:39<00:33, 14.32it/s]\u001b[A\n",
      " 53%|    | 550/1033 [00:39<00:33, 14.51it/s]\u001b[A\n",
      " 53%|    | 552/1033 [00:39<00:32, 14.63it/s]\u001b[A\n",
      " 54%|    | 554/1033 [00:39<00:32, 14.64it/s]\u001b[A\n",
      " 54%|    | 556/1033 [00:39<00:32, 14.53it/s]\u001b[A\n",
      " 54%|    | 558/1033 [00:39<00:32, 14.68it/s]\u001b[A\n",
      " 54%|    | 560/1033 [00:40<00:32, 14.69it/s]\u001b[A\n",
      " 54%|    | 562/1033 [00:40<00:36, 12.82it/s]\u001b[A\n",
      " 55%|    | 564/1033 [00:40<00:35, 13.39it/s]\u001b[A\n",
      " 55%|    | 566/1033 [00:40<00:33, 13.78it/s]\u001b[A\n",
      " 55%|    | 568/1033 [00:40<00:32, 14.17it/s]\u001b[A\n",
      " 55%|    | 570/1033 [00:40<00:32, 14.33it/s]\u001b[A\n",
      " 55%|    | 572/1033 [00:40<00:31, 14.56it/s]\u001b[A\n",
      " 56%|    | 574/1033 [00:41<00:31, 14.63it/s]\u001b[A\n",
      " 56%|    | 576/1033 [00:41<00:31, 14.50it/s]\u001b[A\n",
      " 56%|    | 578/1033 [00:41<00:31, 14.59it/s]\u001b[A\n",
      " 56%|    | 580/1033 [00:41<00:35, 12.68it/s]\u001b[A\n",
      " 56%|    | 582/1033 [00:41<00:33, 13.28it/s]\u001b[A\n",
      " 57%|    | 584/1033 [00:41<00:32, 13.86it/s]\u001b[A\n",
      " 57%|    | 586/1033 [00:42<00:31, 14.14it/s]\u001b[A\n",
      " 57%|    | 588/1033 [00:42<00:32, 13.76it/s]\u001b[A\n",
      " 57%|    | 590/1033 [00:42<00:32, 13.63it/s]\u001b[A\n",
      " 57%|    | 592/1033 [00:42<00:31, 13.93it/s]\u001b[A\n",
      " 58%|    | 594/1033 [00:42<00:30, 14.31it/s]\u001b[A\n",
      " 58%|    | 596/1033 [00:42<00:30, 14.14it/s]\u001b[A\n",
      " 58%|    | 598/1033 [00:42<00:34, 12.56it/s]\u001b[A\n",
      " 58%|    | 600/1033 [00:43<00:32, 13.29it/s]\u001b[A\n",
      " 58%|    | 602/1033 [00:43<00:31, 13.87it/s]\u001b[A\n",
      " 58%|    | 604/1033 [00:43<00:29, 14.31it/s]\u001b[A\n",
      " 59%|    | 606/1033 [00:43<00:29, 14.60it/s]\u001b[A\n",
      " 59%|    | 608/1033 [00:43<00:29, 14.53it/s]\u001b[A\n",
      " 59%|    | 610/1033 [00:43<00:29, 14.54it/s]\u001b[A\n",
      " 59%|    | 612/1033 [00:43<00:28, 14.60it/s]\u001b[A\n",
      " 59%|    | 614/1033 [00:43<00:28, 14.63it/s]\u001b[A\n",
      " 60%|    | 616/1033 [00:44<00:28, 14.62it/s]\u001b[A\n",
      " 60%|    | 618/1033 [00:44<00:32, 12.69it/s]\u001b[A\n",
      " 60%|    | 620/1033 [00:44<00:30, 13.38it/s]\u001b[A\n",
      " 60%|    | 622/1033 [00:44<00:29, 13.75it/s]\u001b[A\n",
      " 60%|    | 624/1033 [00:44<00:29, 13.97it/s]\u001b[A\n",
      " 61%|    | 626/1033 [00:44<00:28, 14.30it/s]\u001b[A\n",
      " 61%|    | 628/1033 [00:44<00:27, 14.66it/s]\u001b[A\n",
      " 61%|    | 630/1033 [00:45<00:27, 14.60it/s]\u001b[A\n",
      " 61%|    | 632/1033 [00:45<00:27, 14.56it/s]\u001b[A\n",
      " 61%|   | 634/1033 [00:45<00:27, 14.71it/s]\u001b[A\n",
      " 62%|   | 636/1033 [00:45<00:31, 12.75it/s]\u001b[A\n",
      " 62%|   | 638/1033 [00:45<00:29, 13.38it/s]\u001b[A\n",
      " 62%|   | 640/1033 [00:45<00:28, 13.73it/s]\u001b[A\n",
      " 62%|   | 642/1033 [00:46<00:28, 13.83it/s]\u001b[A\n",
      " 62%|   | 644/1033 [00:46<00:27, 14.22it/s]\u001b[A\n",
      " 63%|   | 646/1033 [00:46<00:26, 14.64it/s]\u001b[A\n",
      " 63%|   | 648/1033 [00:46<00:26, 14.72it/s]\u001b[A\n",
      " 63%|   | 650/1033 [00:46<00:25, 14.84it/s]\u001b[A\n",
      " 63%|   | 652/1033 [00:46<00:25, 14.81it/s]\u001b[A\n",
      " 63%|   | 654/1033 [00:46<00:27, 13.76it/s]\u001b[A\n",
      " 64%|   | 656/1033 [00:47<00:28, 13.39it/s]\u001b[A\n",
      " 64%|   | 658/1033 [00:47<00:28, 13.29it/s]\u001b[A\n",
      " 64%|   | 660/1033 [00:47<00:26, 13.89it/s]\u001b[A\n",
      " 64%|   | 662/1033 [00:47<00:25, 14.29it/s]\u001b[A\n",
      " 64%|   | 664/1033 [00:47<00:25, 14.61it/s]\u001b[A\n",
      " 64%|   | 666/1033 [00:47<00:24, 14.87it/s]\u001b[A\n",
      " 65%|   | 668/1033 [00:47<00:24, 14.83it/s]\u001b[A\n",
      " 65%|   | 670/1033 [00:47<00:24, 14.92it/s]\u001b[A\n",
      " 65%|   | 672/1033 [00:48<00:24, 15.03it/s]\u001b[A\n",
      " 65%|   | 674/1033 [00:48<00:27, 12.96it/s]\u001b[A\n",
      " 65%|   | 676/1033 [00:48<00:26, 13.55it/s]\u001b[A\n",
      " 66%|   | 678/1033 [00:48<00:25, 13.93it/s]\u001b[A\n",
      " 66%|   | 680/1033 [00:48<00:24, 14.23it/s]\u001b[A\n",
      " 66%|   | 682/1033 [00:48<00:24, 14.26it/s]\u001b[A\n",
      " 66%|   | 684/1033 [00:48<00:24, 14.35it/s]\u001b[A\n",
      " 66%|   | 686/1033 [00:49<00:23, 14.63it/s]\u001b[A\n",
      " 67%|   | 688/1033 [00:49<00:23, 14.72it/s]\u001b[A\n",
      " 67%|   | 690/1033 [00:49<00:23, 14.71it/s]\u001b[A\n",
      " 67%|   | 692/1033 [00:49<00:26, 12.69it/s]\u001b[A\n",
      " 67%|   | 694/1033 [00:49<00:25, 13.25it/s]\u001b[A\n",
      " 67%|   | 696/1033 [00:49<00:24, 13.77it/s]\u001b[A\n",
      " 68%|   | 698/1033 [00:49<00:23, 14.13it/s]\u001b[A\n",
      " 68%|   | 700/1033 [00:50<00:23, 14.30it/s]\u001b[A\n",
      " 68%|   | 702/1033 [00:50<00:23, 14.36it/s]\u001b[A\n",
      " 68%|   | 704/1033 [00:50<00:22, 14.57it/s]\u001b[A\n",
      " 68%|   | 706/1033 [00:50<00:21, 14.94it/s]\u001b[A\n",
      " 69%|   | 708/1033 [00:50<00:21, 14.93it/s]\u001b[A\n",
      " 69%|   | 710/1033 [00:50<00:21, 14.85it/s]\u001b[A\n",
      " 69%|   | 712/1033 [00:50<00:24, 12.91it/s]\u001b[A\n",
      " 69%|   | 714/1033 [00:51<00:23, 13.49it/s]\u001b[A\n",
      " 69%|   | 716/1033 [00:51<00:22, 13.89it/s]\u001b[A\n",
      " 70%|   | 718/1033 [00:51<00:22, 14.13it/s]\u001b[A\n",
      " 70%|   | 720/1033 [00:51<00:21, 14.39it/s]\u001b[A\n",
      " 70%|   | 722/1033 [00:51<00:22, 14.06it/s]\u001b[A\n",
      " 70%|   | 724/1033 [00:51<00:21, 14.20it/s]\u001b[A\n",
      " 70%|   | 726/1033 [00:51<00:21, 14.54it/s]\u001b[A\n",
      " 70%|   | 728/1033 [00:52<00:20, 14.58it/s]\u001b[A\n",
      " 71%|   | 730/1033 [00:52<00:23, 12.73it/s]\u001b[A\n",
      " 71%|   | 732/1033 [00:52<00:22, 13.35it/s]\u001b[A\n",
      " 71%|   | 734/1033 [00:52<00:21, 13.79it/s]\u001b[A\n",
      " 71%|   | 736/1033 [00:52<00:21, 14.11it/s]\u001b[A\n",
      " 71%|  | 738/1033 [00:52<00:20, 14.50it/s]\u001b[A\n",
      " 72%|  | 740/1033 [00:52<00:20, 14.63it/s]\u001b[A\n",
      " 72%|  | 742/1033 [00:53<00:19, 14.67it/s]\u001b[A\n",
      " 72%|  | 744/1033 [00:53<00:19, 14.77it/s]\u001b[A\n",
      " 72%|  | 746/1033 [00:53<00:19, 14.94it/s]\u001b[A\n",
      " 72%|  | 748/1033 [00:53<00:18, 15.12it/s]\u001b[A\n",
      " 73%|  | 750/1033 [00:53<00:21, 13.17it/s]\u001b[A\n",
      " 73%|  | 752/1033 [00:53<00:20, 13.77it/s]\u001b[A\n",
      " 73%|  | 754/1033 [00:53<00:19, 14.17it/s]\u001b[A\n",
      " 73%|  | 756/1033 [00:54<00:19, 14.45it/s]\u001b[A\n",
      " 73%|  | 758/1033 [00:54<00:18, 14.56it/s]\u001b[A\n",
      " 74%|  | 760/1033 [00:54<00:18, 14.74it/s]\u001b[A\n",
      " 74%|  | 762/1033 [00:54<00:18, 14.82it/s]\u001b[A\n",
      " 74%|  | 764/1033 [00:54<00:19, 13.83it/s]\u001b[A\n",
      " 74%|  | 766/1033 [00:54<00:19, 14.03it/s]\u001b[A\n",
      " 74%|  | 768/1033 [00:54<00:21, 12.30it/s]\u001b[A\n",
      " 75%|  | 770/1033 [00:55<00:20, 12.74it/s]\u001b[A\n",
      " 75%|  | 772/1033 [00:55<00:19, 13.30it/s]\u001b[A\n",
      " 75%|  | 774/1033 [00:55<00:19, 13.41it/s]\u001b[A\n",
      " 75%|  | 776/1033 [00:55<00:18, 13.87it/s]\u001b[A\n",
      " 75%|  | 778/1033 [00:55<00:17, 14.25it/s]\u001b[A\n",
      " 76%|  | 780/1033 [00:55<00:17, 14.35it/s]\u001b[A\n",
      " 76%|  | 782/1033 [00:55<00:17, 14.33it/s]\u001b[A\n",
      " 76%|  | 784/1033 [00:56<00:17, 14.52it/s]\u001b[A\n",
      " 76%|  | 786/1033 [00:56<00:19, 12.65it/s]\u001b[A\n",
      " 76%|  | 788/1033 [00:56<00:18, 12.93it/s]\u001b[A\n",
      " 76%|  | 790/1033 [00:56<00:18, 13.42it/s]\u001b[A\n",
      " 77%|  | 792/1033 [00:56<00:17, 13.91it/s]\u001b[A\n",
      " 77%|  | 794/1033 [00:56<00:16, 14.14it/s]\u001b[A\n",
      " 77%|  | 796/1033 [00:56<00:16, 14.39it/s]\u001b[A\n",
      " 77%|  | 798/1033 [00:57<00:16, 14.56it/s]\u001b[A\n",
      " 77%|  | 800/1033 [00:57<00:15, 14.63it/s]\u001b[A\n",
      " 78%|  | 802/1033 [00:57<00:15, 14.81it/s]\u001b[A\n",
      " 78%|  | 804/1033 [00:57<00:17, 12.89it/s]\u001b[A\n",
      " 78%|  | 806/1033 [00:57<00:16, 13.39it/s]\u001b[A\n",
      " 78%|  | 808/1033 [00:57<00:16, 13.90it/s]\u001b[A\n",
      " 78%|  | 810/1033 [00:57<00:15, 13.96it/s]\u001b[A\n",
      " 79%|  | 812/1033 [00:58<00:15, 14.35it/s]\u001b[A\n",
      " 79%|  | 814/1033 [00:58<00:15, 14.50it/s]\u001b[A\n",
      " 79%|  | 816/1033 [00:58<00:14, 14.65it/s]\u001b[A\n",
      " 79%|  | 818/1033 [00:58<00:14, 14.76it/s]\u001b[A\n",
      " 79%|  | 820/1033 [00:58<00:14, 14.79it/s]\u001b[A\n",
      " 80%|  | 822/1033 [00:58<00:14, 14.68it/s]\u001b[A\n",
      " 80%|  | 824/1033 [00:58<00:16, 12.85it/s]\u001b[A\n",
      " 80%|  | 826/1033 [00:59<00:15, 13.50it/s]\u001b[A\n",
      " 80%|  | 828/1033 [00:59<00:14, 14.01it/s]\u001b[A\n",
      " 80%|  | 830/1033 [00:59<00:14, 14.16it/s]\u001b[A\n",
      " 81%|  | 832/1033 [00:59<00:14, 14.35it/s]\u001b[A\n",
      " 81%|  | 834/1033 [00:59<00:13, 14.43it/s]\u001b[A\n",
      " 81%|  | 836/1033 [00:59<00:13, 14.48it/s]\u001b[A\n",
      " 81%|  | 838/1033 [00:59<00:13, 14.57it/s]\u001b[A\n",
      " 81%| | 840/1033 [01:00<00:13, 14.73it/s]\u001b[A\n",
      " 82%| | 842/1033 [01:00<00:14, 12.89it/s]\u001b[A\n",
      " 82%| | 844/1033 [01:00<00:13, 13.53it/s]\u001b[A\n",
      " 82%| | 846/1033 [01:00<00:13, 13.81it/s]\u001b[A\n",
      " 82%| | 848/1033 [01:00<00:13, 14.04it/s]\u001b[A\n",
      " 82%| | 850/1033 [01:00<00:12, 14.39it/s]\u001b[A\n",
      " 82%| | 852/1033 [01:00<00:12, 14.46it/s]\u001b[A\n",
      " 83%| | 854/1033 [01:01<00:12, 14.63it/s]\u001b[A\n",
      " 83%| | 856/1033 [01:01<00:11, 14.77it/s]\u001b[A\n",
      " 83%| | 858/1033 [01:01<00:11, 14.83it/s]\u001b[A\n",
      " 83%| | 860/1033 [01:01<00:13, 12.59it/s]\u001b[A\n",
      " 83%| | 862/1033 [01:01<00:12, 13.17it/s]\u001b[A\n",
      " 84%| | 864/1033 [01:01<00:12, 13.73it/s]\u001b[A\n",
      " 84%| | 866/1033 [01:01<00:11, 14.27it/s]\u001b[A\n",
      " 84%| | 868/1033 [01:02<00:11, 14.34it/s]\u001b[A\n",
      " 84%| | 870/1033 [01:02<00:11, 14.43it/s]\u001b[A\n",
      " 84%| | 872/1033 [01:02<00:10, 14.65it/s]\u001b[A\n",
      " 85%| | 874/1033 [01:02<00:10, 14.94it/s]\u001b[A\n",
      " 85%| | 876/1033 [01:02<00:10, 14.89it/s]\u001b[A\n",
      " 85%| | 878/1033 [01:02<00:10, 14.90it/s]\u001b[A\n",
      " 85%| | 880/1033 [01:02<00:11, 12.82it/s]\u001b[A\n",
      " 85%| | 882/1033 [01:03<00:11, 13.20it/s]\u001b[A\n",
      " 86%| | 884/1033 [01:03<00:10, 13.72it/s]\u001b[A\n",
      " 86%| | 886/1033 [01:03<00:10, 14.13it/s]\u001b[A\n",
      " 86%| | 888/1033 [01:03<00:10, 14.37it/s]\u001b[A\n",
      " 86%| | 890/1033 [01:03<00:09, 14.55it/s]\u001b[A\n",
      " 86%| | 892/1033 [01:03<00:09, 14.71it/s]\u001b[A\n",
      " 87%| | 894/1033 [01:03<00:09, 14.85it/s]\u001b[A\n",
      " 87%| | 896/1033 [01:04<00:09, 14.93it/s]\u001b[A\n",
      " 87%| | 898/1033 [01:04<00:10, 12.84it/s]\u001b[A\n",
      " 87%| | 900/1033 [01:04<00:09, 13.42it/s]\u001b[A\n",
      " 87%| | 902/1033 [01:04<00:09, 13.86it/s]\u001b[A\n",
      " 88%| | 904/1033 [01:04<00:08, 14.40it/s]\u001b[A\n",
      " 88%| | 906/1033 [01:04<00:08, 14.62it/s]\u001b[A\n",
      " 88%| | 908/1033 [01:04<00:08, 14.60it/s]\u001b[A\n",
      " 88%| | 910/1033 [01:05<00:08, 14.77it/s]\u001b[A\n",
      " 88%| | 912/1033 [01:05<00:08, 14.72it/s]\u001b[A\n",
      " 88%| | 914/1033 [01:05<00:08, 14.70it/s]\u001b[A\n",
      " 89%| | 916/1033 [01:05<00:08, 14.08it/s]\u001b[A\n",
      " 89%| | 918/1033 [01:05<00:08, 12.89it/s]\u001b[A\n",
      " 89%| | 920/1033 [01:05<00:08, 13.41it/s]\u001b[A\n",
      " 89%| | 922/1033 [01:05<00:07, 13.96it/s]\u001b[A\n",
      " 89%| | 924/1033 [01:06<00:07, 14.33it/s]\u001b[A\n",
      " 90%| | 926/1033 [01:06<00:07, 14.44it/s]\u001b[A\n",
      " 90%| | 928/1033 [01:06<00:07, 14.73it/s]\u001b[A\n",
      " 90%| | 930/1033 [01:06<00:07, 14.65it/s]\u001b[A\n",
      " 90%| | 932/1033 [01:06<00:06, 14.82it/s]\u001b[A\n",
      " 90%| | 934/1033 [01:06<00:06, 14.82it/s]\u001b[A\n",
      " 91%| | 936/1033 [01:06<00:07, 12.95it/s]\u001b[A\n",
      " 91%| | 938/1033 [01:07<00:07, 13.46it/s]\u001b[A\n",
      " 91%| | 940/1033 [01:07<00:06, 13.76it/s]\u001b[A\n",
      " 91%| | 942/1033 [01:07<00:06, 14.17it/s]\u001b[A\n",
      " 91%|| 944/1033 [01:07<00:06, 14.34it/s]\u001b[A\n",
      " 92%|| 946/1033 [01:07<00:06, 14.48it/s]\u001b[A\n",
      " 92%|| 948/1033 [01:07<00:05, 14.56it/s]\u001b[A\n",
      " 92%|| 950/1033 [01:07<00:05, 14.51it/s]\u001b[A\n",
      " 92%|| 952/1033 [01:07<00:05, 14.64it/s]\u001b[A\n",
      " 92%|| 954/1033 [01:08<00:06, 12.69it/s]\u001b[A\n",
      " 93%|| 956/1033 [01:08<00:05, 13.37it/s]\u001b[A\n",
      " 93%|| 958/1033 [01:08<00:05, 13.78it/s]\u001b[A\n",
      " 93%|| 960/1033 [01:08<00:05, 14.08it/s]\u001b[A\n",
      " 93%|| 962/1033 [01:08<00:04, 14.44it/s]\u001b[A\n",
      " 93%|| 964/1033 [01:08<00:04, 14.57it/s]\u001b[A\n",
      " 94%|| 966/1033 [01:08<00:04, 14.83it/s]\u001b[A\n",
      " 94%|| 968/1033 [01:09<00:04, 14.78it/s]\u001b[A\n",
      " 94%|| 970/1033 [01:09<00:04, 14.85it/s]\u001b[A\n",
      " 94%|| 972/1033 [01:09<00:04, 14.95it/s]\u001b[A\n",
      " 94%|| 974/1033 [01:09<00:04, 12.88it/s]\u001b[A\n",
      " 94%|| 976/1033 [01:09<00:04, 13.54it/s]\u001b[A\n",
      " 95%|| 978/1033 [01:09<00:03, 13.81it/s]\u001b[A\n",
      " 95%|| 980/1033 [01:09<00:03, 14.08it/s]\u001b[A\n",
      " 95%|| 982/1033 [01:10<00:03, 14.19it/s]\u001b[A\n",
      " 95%|| 984/1033 [01:10<00:03, 14.13it/s]\u001b[A\n",
      " 95%|| 986/1033 [01:10<00:03, 14.19it/s]\u001b[A\n",
      " 96%|| 988/1033 [01:10<00:03, 14.45it/s]\u001b[A\n",
      " 96%|| 990/1033 [01:10<00:02, 14.62it/s]\u001b[A\n",
      " 96%|| 992/1033 [01:10<00:03, 12.72it/s]\u001b[A\n",
      " 96%|| 994/1033 [01:11<00:02, 13.42it/s]\u001b[A\n",
      " 96%|| 996/1033 [01:11<00:02, 13.76it/s]\u001b[A\n",
      " 97%|| 998/1033 [01:11<00:02, 14.11it/s]\u001b[A\n",
      " 97%|| 1000/1033 [01:11<00:02, 14.35it/s]\u001b[A\n",
      " 97%|| 1002/1033 [01:11<00:02, 14.53it/s]\u001b[A\n",
      " 97%|| 1004/1033 [01:11<00:01, 14.52it/s]\u001b[A\n",
      " 97%|| 1006/1033 [01:11<00:01, 14.81it/s]\u001b[A\n",
      " 98%|| 1008/1033 [01:11<00:01, 14.91it/s]\u001b[A\n",
      " 98%|| 1010/1033 [01:12<00:01, 12.87it/s]\u001b[A\n",
      " 98%|| 1012/1033 [01:12<00:01, 13.44it/s]\u001b[A\n",
      " 98%|| 1014/1033 [01:12<00:01, 13.82it/s]\u001b[A\n",
      " 98%|| 1016/1033 [01:12<00:01, 14.13it/s]\u001b[A\n",
      " 99%|| 1018/1033 [01:12<00:01, 14.37it/s]\u001b[A\n",
      " 99%|| 1020/1033 [01:12<00:00, 14.49it/s]\u001b[A\n",
      " 99%|| 1022/1033 [01:12<00:00, 14.64it/s]\u001b[A\n",
      " 99%|| 1024/1033 [01:13<00:00, 14.61it/s]\u001b[A\n",
      " 99%|| 1026/1033 [01:13<00:00, 14.83it/s]\u001b[A\n",
      "100%|| 1028/1033 [01:13<00:00, 14.90it/s]\u001b[A\n",
      "100%|| 1030/1033 [01:13<00:00, 12.71it/s]\u001b[A\n",
      "100%|| 1032/1033 [01:13<00:00, 14.00it/s]\u001b[A\n",
      "/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n",
      "  0%|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 0/2 [01:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 142.62 MiB is free. Process 693901 has 10.43 GiB memory in use. Including non-PyTorch memory, this process has 28.98 GiB memory in use. Of the allocated memory 27.74 GiB is allocated by PyTorch, and 732.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 125\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blurry_recon:     \n\u001b[1;32m    123\u001b[0m     image_enc_pred, transformer_feats \u001b[38;5;241m=\u001b[39m blurry_image_enc_\n\u001b[0;32m--> 125\u001b[0m     image_enc \u001b[38;5;241m=\u001b[39m \u001b[43mautoenc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlatent_dist\u001b[38;5;241m.\u001b[39mmode() \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.18215\u001b[39m\n\u001b[1;32m    126\u001b[0m     loss_blurry \u001b[38;5;241m=\u001b[39m l1(image_enc_pred, image_enc)\n\u001b[1;32m    127\u001b[0m     loss_blurry_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss_blurry\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/diffusers/models/autoencoder_kl.py:274\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[0;34m(self, x, return_dict)\u001b[0m\n\u001b[1;32m    272\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(encoded_slices)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m moments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(h)\n\u001b[1;32m    277\u001b[0m posterior \u001b[38;5;241m=\u001b[39m DiagonalGaussianDistribution(moments)\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/diffusers/models/vae.py:165\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# down\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m down_block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdown_blocks:\n\u001b[0;32m--> 165\u001b[0m         sample \u001b[38;5;241m=\u001b[39m \u001b[43mdown_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# middle\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmid_block(sample)\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/diffusers/models/unet_2d_blocks.py:1323\u001b[0m, in \u001b[0;36mDownEncoderBlock2D.forward\u001b[0;34m(self, hidden_states, scale)\u001b[0m\n\u001b[1;32m   1321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mFloatTensor, scale: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor:\n\u001b[1;32m   1322\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m resnet \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnets:\n\u001b[0;32m-> 1323\u001b[0m         hidden_states \u001b[38;5;241m=\u001b[39m \u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1326\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m downsampler \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdownsamplers:\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/diffusers/models/resnet.py:739\u001b[0m, in \u001b[0;36mResnetBlock2D.forward\u001b[0;34m(self, input_tensor, temb, scale)\u001b[0m\n\u001b[1;32m    737\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(hidden_states, temb)\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 739\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m temb \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_embedding_norm \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_shift\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    742\u001b[0m     scale, shift \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mchunk(temb, \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/modules/normalization.py:279\u001b[0m, in \u001b[0;36mGroupNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/pr2/projects/plgrid/plggrai/kzrobek/MindEyeV2/fmri/lib64/python3.9/site-packages/torch/nn/functional.py:2558\u001b[0m, in \u001b[0;36mgroup_norm\u001b[0;34m(input, num_groups, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2556\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected at least 2 dimensions for input tensor but received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2557\u001b[0m _verify_batch_size([\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_groups, num_groups] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m2\u001b[39m:]))\n\u001b[0;32m-> 2558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroup_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_groups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 392.00 MiB. GPU 0 has a total capacty of 39.56 GiB of which 142.62 MiB is free. Process 693901 has 10.43 GiB memory in use. Including non-PyTorch memory, this process has 28.98 GiB memory in use. Of the allocated memory 27.74 GiB is allocated by PyTorch, and 732.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "print(f\"{model_name} starting with epoch {epoch} / {num_epochs}\")\n",
    "progress_bar = tqdm(range(epoch,num_epochs), ncols=1200, disable=(local_rank!=0))\n",
    "test_image, test_eeg = None, None\n",
    "mse = nn.MSELoss()\n",
    "l1 = nn.L1Loss()\n",
    "soft_loss_temps = utils.cosine_anneal(0.004, 0.0075, num_epochs - int(mixup_pct * num_epochs))\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    model.train()\n",
    "    \n",
    "    fwd_percent_correct = 0.\n",
    "    bwd_percent_correct = 0.\n",
    "    test_fwd_percent_correct = 0.\n",
    "    test_bwd_percent_correct = 0.\n",
    "    \n",
    "    recon_cossim = 0.\n",
    "    test_recon_cossim = 0.\n",
    "    recon_mse = 0.\n",
    "    test_recon_mse = 0.\n",
    "    \n",
    "    loss_clip_total = 0.\n",
    "    loss_blurry_total = 0.\n",
    "    loss_blurry_cont_total = 0.\n",
    "    test_loss_clip_total = 0.\n",
    "    \n",
    "    loss_prior_total = 0.\n",
    "    test_loss_prior_total = 0.\n",
    "    \n",
    "    blurry_pixcorr = 0.\n",
    "    test_blurry_pixcorr = 0. # needs >.456 to beat low-level subj01 results in mindeye v1\n",
    "    \n",
    "    # pre-load all batches for this epoch (it's MUCH faster to pre-load in bulk than to separate loading per batch)\n",
    "    eeg_iters = {}\n",
    "    image_iters = torch.zeros(num_iterations_per_epoch, batch_size*len(subj_list), 3, 224, 224).float()\n",
    "    annot_iters = {}\n",
    "    perm_iters, betas_iters, select_iters = {}, {}, {}\n",
    "    for s, train_dl in enumerate(train_dls):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            iter = -1\n",
    "            for eeg, imgs in tqdm(train_dl): \n",
    "                iter += 1\n",
    "                # Load images \n",
    "                image_tensor = torch.Tensor(imgs)\n",
    "                image_iters[iter, s*batch_size:s*batch_size+batch_size] = image_tensor\n",
    "                \n",
    "                # Load eeg for current batch\n",
    "                \n",
    "                eeg_tensor = torch.Tensor(eeg)\n",
    "    \n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    eeg_tensor, perm, betas, select = utils.mixco(eeg_tensor)\n",
    "                    \n",
    "                    perm_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = perm\n",
    "                    betas_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = betas\n",
    "                    select_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = select\n",
    "    \n",
    "                eeg_iters[f\"subj0{subj_list[s]}_iter{iter}\"] = eeg_tensor\n",
    "    \n",
    "                if iter >= num_iterations_per_epoch-1:\n",
    "                    break\n",
    "\n",
    "\n",
    "    # # you now have eeg_iters and image_iters with num_iterations_per_epoch batches each\n",
    "    for train_i in range(num_iterations_per_epoch):\n",
    "        with torch.cuda.amp.autocast(dtype=data_type):\n",
    "            optimizer.zero_grad()\n",
    "            loss=0.\n",
    "    \n",
    "            eeg_list = [eeg_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "            image = image_iters[train_i].detach()\n",
    "            image = image.to(device)\n",
    "    \n",
    "            if use_image_aug: \n",
    "                image = img_augment(image)\n",
    "    \n",
    "            clip_target = clip_img_embedder(image)\n",
    "            assert not torch.any(torch.isnan(clip_target))\n",
    "    \n",
    "            if epoch < int(mixup_pct * num_epochs):\n",
    "                perm_list = [perm_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                perm = torch.cat(perm_list, dim=0)\n",
    "                betas_list = [betas_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                betas = torch.cat(betas_list, dim=0)\n",
    "                select_list = [select_iters[f\"subj0{s}_iter{train_i}\"].detach().to(device) for s in subj_list]\n",
    "                select = torch.cat(select_list, dim=0)\n",
    "    \n",
    "            eeg_ridge_list = [model.ridge(eeg_list[si].view(batch_size, -1).unsqueeze(1),si) for si,s in enumerate(subj_list)]\n",
    "            eeg_ridge = torch.cat(eeg_ridge_list, dim=0)\n",
    "    \n",
    "            backbone, clip_eegs, blurry_image_enc_ = model.backbone(eeg_ridge)\n",
    "    \n",
    "            if clip_scale>0:\n",
    "                clip_eegs_norm = nn.functional.normalize(clip_eegs.flatten(1), dim=-1)\n",
    "                clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "    \n",
    "            if use_prior:\n",
    "                loss_prior, prior_out = model.diffusion_prior(text_embed=backbone, image_embed=clip_target)\n",
    "                loss_prior_total += loss_prior.item()\n",
    "                loss_prior *= prior_scale\n",
    "                loss += loss_prior\n",
    "    \n",
    "                recon_cossim += nn.functional.cosine_similarity(prior_out, clip_target).mean().item()\n",
    "                recon_mse += mse(prior_out, clip_target).item()\n",
    "            if clip_scale>0:\n",
    "                if epoch < int(mixup_pct * num_epochs):    \n",
    "                    loss_clip = utils.mixco_nce(\n",
    "                        clip_eegs_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006,\n",
    "                        perm=perm, betas=betas, select=select)\n",
    "                else:\n",
    "                    epoch_temp = soft_loss_temps[epoch-int(mixup_pct*num_epochs)]\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_eegs_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=epoch_temp)\n",
    "                loss_clip_total += loss_clip.item()\n",
    "                loss_clip *= clip_scale\n",
    "                loss += loss_clip\n",
    "            \n",
    "                \n",
    "            if blurry_recon:     \n",
    "                image_enc_pred, transformer_feats = blurry_image_enc_\n",
    "    \n",
    "                image_enc = autoenc.encode(2*image-1).latent_dist.mode() * 0.18215\n",
    "                loss_blurry = l1(image_enc_pred, image_enc)\n",
    "                loss_blurry_total += loss_blurry.item()\n",
    "    \n",
    "                if epoch < int(mixup_pct * num_epochs):\n",
    "                    image_enc_shuf = image_enc[perm]\n",
    "                    betas_shape = [-1] + [1]*(len(image_enc.shape)-1)\n",
    "                    image_enc[select] = image_enc[select] * betas[select].reshape(*betas_shape) + \\\n",
    "                        image_enc_shuf[select] * (1 - betas[select]).reshape(*betas_shape)\n",
    "    \n",
    "                image_norm = (image - mean)/std\n",
    "                image_aug = (blur_augs(image) - mean)/std\n",
    "                _, cnx_embeds = cnx(image_norm)\n",
    "                _, cnx_aug_embeds = cnx(image_aug)\n",
    "    \n",
    "                cont_loss = utils.soft_cont_loss(\n",
    "                    nn.functional.normalize(transformer_feats.reshape(-1, transformer_feats.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    nn.functional.normalize(cnx_aug_embeds.reshape(-1, cnx_embeds.shape[-1]), dim=-1),\n",
    "                    temp=0.2)\n",
    "                loss_blurry_cont_total += cont_loss.item()\n",
    "    \n",
    "                loss += (loss_blurry + 0.1*cont_loss) * blur_scale #/.18215\n",
    "    \n",
    "            if clip_scale>0:\n",
    "                # forward and backward top 1 accuracy        \n",
    "                labels = torch.arange(len(clip_eegs_norm)).to(clip_eegs_norm.device) \n",
    "                fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_eegs_norm, clip_target_norm), labels, k=1).item()\n",
    "                bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_eegs_norm), labels, k=1).item()\n",
    "    \n",
    "            if blurry_recon:\n",
    "                with torch.no_grad():\n",
    "                    # only doing pixcorr eval on a subset of the samples per batch because its costly & slow to compute autoenc.decode()\n",
    "                    random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample/ 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    blurry_pixcorr += pixcorr.item()\n",
    "                \n",
    "            utils.check_loss(loss)\n",
    "            accelerator.backward(loss)\n",
    "            optimizer.step()\n",
    "    \n",
    "            losses.append(loss.item())\n",
    "            lrs.append(optimizer.param_groups[0]['lr'])\n",
    "    \n",
    "            if lr_scheduler_type is not None:\n",
    "                lr_scheduler.step()\n",
    "    model.eval()\n",
    "    \n",
    "    if local_rank==0:\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast(dtype=data_type): \n",
    "            test_i = -1\n",
    "            for eeg, imgs in tqdm(test_dl):  \n",
    "                test_i += 1\n",
    "                test_eeg = torch.Tensor(eeg).to(device)\n",
    "                test_image = torch.Tensor(imgs).float().to(device)\n",
    "                loss=0.\n",
    "    \n",
    "                clip_target = clip_img_embedder(test_image)\n",
    "                for rep in range(3):\n",
    "                    eeg_ridge = model.ridge(test_eeg.view(batch_size,-1).unsqueeze(1),0) # 0th index of subj_list\n",
    "                    backbone0, clip_eegs0, blurry_image_enc_ = model.backbone(eeg_ridge)\n",
    "                    if rep==0:\n",
    "                        clip_eegs = clip_eegs0\n",
    "                        backbone = backbone0\n",
    "                    else:\n",
    "                        clip_eegs += clip_eegs0\n",
    "                        backbone += backbone0\n",
    "                clip_eegs /= 3\n",
    "                backbone /= 3\n",
    "    \n",
    "                if clip_scale>0:\n",
    "                    clip_eegs_norm = nn.functional.normalize(clip_eegs.flatten(1), dim=-1)\n",
    "                    clip_target_norm = nn.functional.normalize(clip_target.flatten(1), dim=-1)\n",
    "                \n",
    "                # for some evals, only doing a subset of the samples per batch because of computational cost\n",
    "                random_samps = np.random.choice(np.arange(len(image)), size=len(image)//5, replace=False)\n",
    "                \n",
    "                if use_prior:\n",
    "                    loss_prior, contaminated_prior_out = model.diffusion_prior(text_embed=backbone[random_samps], image_embed=clip_target[random_samps])\n",
    "                    test_loss_prior_total += loss_prior.item()\n",
    "                    loss_prior *= prior_scale\n",
    "                    loss += loss_prior\n",
    "                    \n",
    "                if clip_scale>0:\n",
    "                    loss_clip = utils.soft_clip_loss(\n",
    "                        clip_eegs_norm,\n",
    "                        clip_target_norm,\n",
    "                        temp=.006)\n",
    "    \n",
    "                    test_loss_clip_total += loss_clip.item()\n",
    "                    loss_clip = loss_clip * clip_scale\n",
    "                    loss += loss_clip\n",
    "                \n",
    "                if blurry_recon:\n",
    "                    image_enc_pred, _ = blurry_image_enc_\n",
    "                    blurry_recon_images = (autoenc.decode(image_enc_pred[random_samps]/0.18215).sample / 2 + 0.5).clamp(0,1)\n",
    "                    pixcorr = utils.pixcorr(image[random_samps], blurry_recon_images)\n",
    "                    test_blurry_pixcorr += pixcorr.item()\n",
    "    \n",
    "                if clip_scale>0:\n",
    "                    # forward and backward top 1 accuracy        \n",
    "                    labels = torch.arange(len(clip_eegs_norm)).to(clip_eegs_norm.device) \n",
    "                    test_fwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_eegs_norm, clip_target_norm), labels, k=1).item()\n",
    "                    test_bwd_percent_correct += utils.topk(utils.batchwise_cosine_similarity(clip_target_norm, clip_eegs_norm), labels, k=1).item()\n",
    "                \n",
    "                utils.check_loss(loss)                \n",
    "                test_losses.append(loss.item())\n",
    "    \n",
    "         \n",
    "            logs = {\"train/loss\": np.mean(losses[-(train_i+1):]),\n",
    "                \"test/loss\": np.mean(test_losses[-(test_i+1):]),\n",
    "                \"train/num_steps\": len(losses),\n",
    "                \"test/num_steps\": len(test_losses),\n",
    "                \"train/fwd_pct_correct\": fwd_percent_correct / (train_i + 1),\n",
    "                \"train/bwd_pct_correct\": bwd_percent_correct / (train_i + 1),\n",
    "                \"test/test_fwd_pct_correct\": test_fwd_percent_correct / (test_i + 1),\n",
    "                \"test/test_bwd_pct_correct\": test_bwd_percent_correct / (test_i + 1),\n",
    "                \"train/loss_clip_total\": loss_clip_total / (train_i + 1),\n",
    "                \"train/loss_blurry_total\": loss_blurry_total / (train_i + 1),\n",
    "                \"train/loss_blurry_cont_total\": loss_blurry_cont_total / (train_i + 1),\n",
    "                \"test/loss_clip_total\": test_loss_clip_total / (test_i + 1),\n",
    "                \"train/blurry_pixcorr\": blurry_pixcorr / (train_i + 1),\n",
    "                \"test/blurry_pixcorr\": test_blurry_pixcorr / (test_i + 1),\n",
    "                \"train/recon_cossim\": recon_cossim / (train_i + 1),\n",
    "                \"test/recon_cossim\": test_recon_cossim / (test_i + 1),\n",
    "                \"train/recon_mse\": recon_mse / (train_i + 1),\n",
    "                \"test/recon_mse\": test_recon_mse / (test_i + 1),\n",
    "                \"train/loss_prior\": loss_prior_total / (train_i + 1),\n",
    "                \"test/loss_prior\": test_loss_prior_total / (test_i + 1),\n",
    "                }\n",
    "    \n",
    "            # if finished training, save jpg recons if they exist\n",
    "            if (epoch == num_epochs-1) or (epoch % ckpt_interval == 0):\n",
    "                if blurry_recon:    \n",
    "                    image_enc = autoenc.encode(2*image[:4]-1).latent_dist.mode() * 0.18215\n",
    "                    # transform blurry recon latents to images and plot it\n",
    "                    fig, axes = plt.subplots(1, 8, figsize=(10, 4))\n",
    "                    jj=-1\n",
    "                    for j in [0,1,2,3]:\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "                        jj+=1\n",
    "                        axes[jj].imshow(utils.torch_to_Image((autoenc.decode(image_enc_pred[[j]]/0.18215).sample / 2 + 0.5).clamp(0,1)))\n",
    "                        axes[jj].axis('off')\n",
    "    \n",
    "                    if wandb_log:\n",
    "                        logs[f\"test/blur_recons\"] = wandb.Image(fig, caption=f\"epoch{epoch:03d}\")\n",
    "                        plt.close()\n",
    "                    else:\n",
    "                        plt.show()\n",
    "    \n",
    "            progress_bar.set_postfix(**logs)\n",
    "    \n",
    "            if wandb_log: wandb.log(logs)\n",
    "            \n",
    "    # Save model checkpoint and reconstruct\n",
    "    if (ckpt_saving) and (epoch % ckpt_interval == 0):\n",
    "        save_ckpt(f'last')\n",
    "    \n",
    "    # wait for other GPUs to catch up if needed\n",
    "    accelerator.wait_for_everyone()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "print(\"\\n===Finished!===\\n\")\n",
    "if ckpt_saving:\n",
    "    save_ckpt(f'last')    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81ae3-171f-40ad-a3e8-24bee4472325",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.show()\n",
    "plt.plot(test_losses)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd90351-4cf7-468d-9a86-f87cd7830542",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fmri",
   "language": "python",
   "name": "fmri"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "vscode": {
   "interpreter": {
    "hash": "62aae01ef0cf7b6af841ab1c8ce59175c4332e693ab3d00bc32ceffb78a35376"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
